# Chapter 4: Process Address Space

* One of the major advantages of virtual memory is that each process has its own
  virtual address space, mapped to physical memory by the operating system.

* This chapter explores the address space as seen by a process, and how it is
  managed by linux.

* The kernel treats the userspace portion of the address space very differently
  from the kernel portion. For example, allocations for the kernel are satisfied
  immediately and are visible globally, no matter which process is current.

* An exception to this however is [vmalloc()][vmalloc] (and consequently
  [__vmalloc()][__vmalloc]), as it causes a minor page fault to occur to
  synchronise the process page tables with the reference page tables, however
  the page will still be allocated immediately upon request.

* For a process, space is simply reserved in the linear address space by
  pointing a page table entry to a read-only globally visible page filled with
  zeros.

* When the process tries to write to this table a page fault is triggered
  causing the kernel to allocate a new zeroed page and assign it to the PTE and
  mark it writeable. It's zeroed so it appears precisely the same as the global
  zero-filled page.

* The userspace portion of virtual memory is not trusted nor presumed
  constant. After each context switch, the userspace portion of the linear
  address space can change except when a 'Lazy TLB' switch is used (discussed in
  4.3.)

* As a result, the kernel has to be configured to catch all exceptions and
  address errors raised from userspace (discussed in 4.5.)

## 4.1 Linear Address Space

* From a user perspective, the address space is a flat, linear address
  space. The kernel's view is rather different - the address space is split
  between userspace which potentially changes on context switch and the kernel
  address space which remains constant.

* The split is determined by the value of [PAGE_OFFSET][PAGE_OFFSET] (==
  [__PAGE_OFFSET][__PAGE_OFFSET]) - 0xc0000000 on i386 - meaning that 3GiB is
  available for the process to use while the remaining 1GiB is always mapped by
  the kernel.

* Diagramatically the kernel address space looks as follows:

```
            0 -> |-----------------|                  ^                 ^
                 |     Process     |                  |                 |
                 |     Address     |                  |                 |
                 |      Space      |                  |                 |
                 /        .        /                  | TASK_SIZE       |
                 \        .        \                  |                 |
                 /        .        /                  |                 |
                 |                 |                  |                 |
  PAGE_OFFSET -> |-----------------|                  X                 |
                 |      Kernel     |                  | Physical        |
                 |      Image      |                  | Memory Map      |
                 |-----------------|                  |                 |
                 |   struct page   |                  | (Depends on #   | Linear Address
                 |  Map (mem_map)  |                  |  physical RAM)  | Space
                 |-----------------| ^                v                 | (2^BITS_PER_LONG bytes)
                 |    Pages Gap    | | VMALLOC_OFFSET                   |
VMALLOC_START -> |-----------------| v                ^                 |
                 |     vmalloc     |                  |                 |
                 |  Address Space  |                  |                 |
  VMALLOC_END -> |-----------------| ^                |                 |
                 |    Pages Gap    | | 2 * PAGE_SIZE  |                 |
   PKMAP_BASE -> |-----------------| X                |                 |
                 |      kmap       | | LAST_PKMAP *   | VMALLOC_RESERVE |
                 |  Address Space  | | PAGE_SIZE      | at minimum      |
FIXADDR_START -> |-----------------| X                |                 |
                 |  Fixed Virtual  | | __FIXADDR_SIZE |                 |
                 | Address Mapping | |                |                 |
  FIXADDR_TOP -> |-----------------| v                |                 |
                 |    Page Gap     |                  |                 |
                 |-----------------|                  v                 v
```

* To load the kernel image, 8MiB (the amount of memory addressed by two PGDs) is
  reserved at `PAGE_OFFSET`. The kernel image is placed in this reserved space
  during kernel page table initialisation as discussed in 3.6.1.

* Somewhere shortly after the image, the [mem_map][mem_map] for UMA
  architectures (as discussed in chapter 2) is stored. The location is usually at
  the 16MiB mark to avoid using `ZONE_DMA`, but not always.

* For NUMA architectures, portions of the virtual `mem_map` will be scattered
  throughout this region and where they are actually located is architecture
  dependent.

* The region between `PAGE_OFFSET` and `VMALLOC_START - VMALLOC_OFFSET`, is the
  'physical memory map' and the size of the region depends on the amount of
  available physical RAM.

* Between the physical memory map and the vmalloc address space there is a gap
  `VMALLOC_OFFSET` in size (8MiB on i386) used to guard against out-of-bound
  errors.

* As an example, an i386 system with 32MiB of RAM with have `VMALLOC_START`
  located at `PAGE_OFFSET + 0x02000000 + 0x00800000` (i.e. `PAGE_OFFSET` +
  32MiB + 8MiB.)

* In low-memory systems, the remaining amount of the virtual address space,
  minus a 2 page gap, is used by [vmalloc()][vmalloc] for representing
  non-contiguous memory allocations in a contiguous virtual address space.

* In high-memory systems, the `vmalloc` area extends as far as `PKMAP_BASE`
  minus the two-page gap, and two extra regions are introduced - kmap and fixed
  virtual address mappings.

* The `kmap` region, which begins at [PKMAP_BASE][PKMAP_BASE], is reserved for
  the mapping of high memory pages into low memory via [kmap()][kmap] (and
  subsequently [__kmap()][__kmap].) We'll go into this in more detail in chapter
  9.

* The fixed virtual address mapping region, which begins at
  [FIXADDR_START][FIXADDR_START] and ends at [FIXADDR_TOP][FIXADDR_TOP], is used
  by subsystems that need to know the virtual address of a mapping at compile
  time, e.g. [APIC][apic] mappings.

* On i386, `FIXADDR_TOP` is statically defined to be `0xffffe000`, which is one
  page prior to the end of the virtual address space. The size of this region is
  calculated at compile time via [__FIXADDR_SIZE][__FIXADDR_SIZE] and used to
  index back from `FIXADDR_TOP` to give the start of the region,
  `FIXADDR_START`.

* The region required for [vmalloc()][vmalloc], [kmap()][kmap] and the fixed
virtual address mappings is what limits the size of `ZONE_NORMAL`.

* As the running kernel requires these functions, a region of at least
  [VMALLOC_RESERVE][VMALLOC_RESERVE] (which is aliased to
  [__VMALLOC_RESERVE][__VMALLOC_RESERVE]) is reserved at the top of the address
  space.

* `VMALLOC_RESERVE` is architecture-dependent, but on i386 it's defined as
  128MiB. This explains why `ZONE_NORMAL` is generally considered to be only
  896MiB in size - it's the 1GiB of the upper portion of the linear address
  space minus the minimum 128MiB that is reserved for the vmalloc region.

### 4.2 Managing the Address Space

* The address space that is usable by a process is managed by a high level
  [mm_struct][mm_struct].

* Each address space consists of a a number of page-aligned regions of memory
  that are in use.

* They never overlap, and represent a set of addresses which contain pages that
  are related to each other in protection and purpose.

* The regions are represented by a [struct vm_area_struct][vm_area_struct]:

```c
/*
 * This struct defines a memory VMM memory area. There is one of these
 * per VM-area/task.  A VM area is any part of the process virtual memory
 * space that has a special rule for the page-fault handlers (ie a shared
 * library, the executable area etc).
 */
struct vm_area_struct {
        struct mm_struct * vm_mm;       /* The address space we belong to. */
        unsigned long vm_start;         /* Our start address within vm_mm. */
        unsigned long vm_end;           /* The first byte after our end address
                                           within vm_mm. */

        /* linked list of VM areas per task, sorted by address */
        struct vm_area_struct *vm_next;

        pgprot_t vm_page_prot;          /* Access permissions of this VMA. */
        unsigned long vm_flags;         /* Flags, listed below. */

        rb_node_t vm_rb;

        /*
         * For areas with an address space and backing store,
         * one of the address_space->i_mmap{,shared} lists,
         * for shm areas, the list of attaches, otherwise unused.
         */
        struct vm_area_struct *vm_next_share;p
        struct vm_area_struct **vm_pprev_share;

        /* Function pointers to deal with this struct. */
        struct vm_operations_struct * vm_ops;

        /* Information about our backing store: */
        unsigned long vm_pgoff;         /* Offset (within vm_file) in PAGE_SIZE
                                           units, *not* PAGE_CACHE_SIZE */
        struct file * vm_file;          /* File we map to (can be NULL). */
        unsigned long vm_raend;         /* XXX: put full readahead info here. */
        void * vm_private_data;         /* was vm_pte (shared mem) */
};
```

* A region might represent the process help for use with `malloc()`, a memory
  mapped file such as a shared library or some `mmap()`-ed memory.

* The pages for the region might be active and resident, paged out or even yet
  to be allocated.

* If a region is backed by a file its `vm_file` field will be set. By traversing
  `vm_file->f_dentry->d_inode->i_mapping`, the associated `address_space` for
  the region may be obtained. The `address_space` has all the
  filesystem-specific information needed to perform page-based operations on
  disk.

* The relationship between different address space-related structures
represented diagrammatically:

```
     |------------------|      |------------------|      |------------------|
- -> | struct mm_struct | ---> | struct mm_struct | ---> | struct mm_struct | - ->
     |------------------|      |------------------|      |------------------|
                                  mmap | ^
                                       | |
                 /---------------------/ \---------------------\
                 v                                             | vm_mm
     |-----------------------|       vm_next        |-----------------------| vm_next
     | struct vm_area_struct | -------------------> | struct vm_area_struct | - ->
     |-----------------------|                      |-----------------------|
                 ^    | vm_file                                 | vm_ops
                 |    |                                         |
                 |    \-----------------\                       |
                 |                      |                       |
                 |                      v                       |
                 |               |-------------|                |
                 |               | struct file |                |
                 |               |-------------|                |
                 |             f_dentry |                       v
                 |                      |       |-----------------------------|
                 |                      |       | struct vm_operations_struct |
                 |                      |       |-----------------------------|
                 |                      v
                 |              |---------------|
                 |              | struct dentry |
                 |              |---------------|
                 |                  ^      | d_inode
                 |                  |      |
                 |         i_dentry |      v
                 |              |--------------|
                 |              | struct inode |
                 |              |--------------|
                 |                       | i_mapping
                 |                       |
                 |                       v
                 |         |----------------------|
                 |         | struct address_space |
                 |         |----------------------|
                 |     i_mmap |     ^          | a_ops
                 \------------/     |          \-----------\
                                    |                      v
                                    |     |----------------------------------|
                                    |     |  struct address_space_operations |
                                    |     |----------------------------------|
                                    |
                            /-------X--------\- - -
                            |                |
                    mapping |                | mapping
                     |-------------|  |-------------|
                     | struct page |  | struct page |
                     |-------------|  |-------------|
```

* There are a number of system calls that affect address space and regions:

1. [fork()][fork] - Creates a new process with a new address space. All the
   pages are marked [Copy-On-Write (COW)][cow] and are shared between the two
   processes until a page fault occurs. Once a write-fault occurs, a copy is
   made of the COW page for the faulting process. This is sometimes referred to
   as 'breaking a COW page'.

2. [clone()][clone] - Similar to `fork()`, however allows context to be shared
   with its parent if the `CLONE_VM` flag is set - this is how linux implements
   threading.

3. [mmap()][mmap] - Creates a new region within the process linear address
   space.

4. [mremap()][mremap] - Remaps or resizes a region of memory. If the virtual
   address space is not available for mapping, the region may be moved, unless
   forbidden by the caller.

5. [munmap()][munmap] - Destroys part or all of a region. If the region being
   unmapped is in the middle of an existing region, the existing region is split
   into two separate regions.

6. [shmat()][shmat] - Attaches a shared memory segment to a process address
   space.

7. [shmdt()][shmdt] - Removes a shared memory segment from a process address
   space.

8. [execve()][execve] - Loads a new executable file and replaces the existing
   address space.

9. [exit()][exit] - Destroys an address space and all its regions.

## 4.3 Process Address Space Descriptor

* The process address space is described by [struct mm_struct][mm_struct],
  meaning that there is only one for each process and it is shared between
  userspace threads.

* Threads are in fact identified in the task list by finding all
  [struct task_struct][task_struct]s that have pointers to the same `mm_struct`.

* Each `task_struct` contains all the information the kernel needs about a
  process.

* A unique `mm_struct` is not needed for kernel threads because they will never
  page fault or access the userspace portion (LS - really? Surely
  `copy_to/from_user()`?)

* An exception to this is page faulting within the `vmalloc` space - the page
  fault handling code treats this as a special case and updates the current page
  table with information in the master page table (LS - what information?
  Vague. What's the 'master' page table?)

* Since an `mm_struct` is therefore not needed for kernel threads, the
  `task_struct->mm` field for kernel threads is always `NULL`.

* For some tasks, such as the boot idle task, the `mm_struct` is never set up,
  but, for kernel threads, a call to [daemonize()][daemonize] (LS - no longer a
  function in recent kernels) will call [exit_mm()][exit_mm] (an alias to
  [__exit_mm()][__exit_mm]) to decrement the usage counter.

* Because TLB flushes are extremely expensive (though read a message from Linus
  on [TLB fill performance][linus-tlb] for some interesting information on
  Intel's optimisations in this area), esp. for architectures such as PPC, a
  technique called 'lazy TLB' is employed.

* Lazy TLB avoids unnecessary TLB flushes by processes that do not access the
  userspace page tables because the kernel portion of the address space is
  always visible. The call to [switch_mm()][switch_mm] which ordinarily results
  in a TLB flush is avoided by borrowing the `mm_struct` used by the previous
  task and placing it in `task_struct->active_mm`. This technique has made large
  improvements to context switch times.

* When entering a lazy TLB, the function [enter_lazy_tlb()][enter_lazy_tlb] is
  called to ensure that a `mm_struct` is not shared between processors in SMP
  machines - it's a null operation on UP machines.

* During process exit [start_lazy_tlb()][start_lazy_tlb] is used birefly while
  the process is waiting to be reaped by the parent.

* Let's take a look at [struct mm_struct][mm_struct] again:

```c
struct mm_struct {
        struct vm_area_struct * mmap;           /* list of VMAs */
        rb_root_t mm_rb;
        struct vm_area_struct * mmap_cache;     /* last find_vma result */
        pgd_t * pgd;
        atomic_t mm_users;                      /* How many users with user space? */
        atomic_t mm_count;                      /* How many references to "struct mm_struct" (users count as 1) */
        int map_count;                          /* number of VMAs */
        struct rw_semaphore mmap_sem;
        spinlock_t page_table_lock;             /* Protects task page tables and mm->rss */

        struct list_head mmlist;                /* List of all active mm's.  These are globally strung
                                                 * together off init_mm.mmlist, and are protected
                                                 * by mmlist_lock
                                                 */

        unsigned long start_code, end_code, start_data, end_data;
        unsigned long start_brk, brk, start_stack;
        unsigned long arg_start, arg_end, env_start, env_end;
        unsigned long rss, total_vm, locked_vm;
        unsigned long def_flags;
        unsigned long cpu_vm_mask;
        unsigned long swap_address;

        unsigned dumpable:1;

        /* Architecture-specific MM context */
        mm_context_t context;
};
```

* Looking at each field:

1. `mmap` - The head of a linked list of all VMA regions in the address space.

2. `mm_rb` - The VMAs are arranged in a linked list and in a
   [red-black tree][rb-tree] for fast lookups - this is the head of the tree.

3. `mmap_cache` - The VMA found during the last call to [find_vma()][find_vma]
   is stored in this field, on the (usually fairly safe) assumption that the
   area will be used again soon.

4. `pgd` - The PGD for this process.

5. `mm_users` - Reference count of processes accessing the userspace portion of
   this `mm_struct` such as page tables and file mappings. Threads and the
   [swap_out()][swap_out] code, for example, will increment this count and make
   sure an `mm_struct` is not destroyed early. When it drops to 0,
   [exit_mmap()][exit_mmap] will delete all (userspace) mappings and tear down
   the page tables before decrementing `mm_count`.

6. `mm_count` - Reference count of the 'anonymous users' for the `mm_struct`,
   initialised at 1 for the real user. An anonymous user is one that does not
   necessarily care about the userspace portion and is just 'borrowing' the
   `mm_struct`, for example kernel threads that use lazy TLB switching. When
   this count drops to 0, the `mm_struct` can be safely destroyed. We need this
   reference count as well as `mm_users` because anonymous users need
   `mm_struct` to exist even if the userspace mappings get destroyed, and there
   is no point in delaying the teardown of the page tables.

7. `map_count` - Number of VMAs in use.

8. `mmap_sem` - A long-lived lock that protects the VMA list for readers and
   writers. Since users of this lock need it for a long time and may sleep, a
   spinlock is inappropriate. A reader of the list takes this semaphore with
   [down_read()][down_read], and a writer with [down_write()][down_write] before
   taking the `page_table_lock` spinlock when the VMA linked lists are being
   updated.

9. `page_table_lock` - Protects most fields on the `mm_struct`. As well as the
   page tables, it protects the Resident Set Size (RSS) count, `rss`, and the
   VMA from modification.

10. `mmlist` - [struct list_head][list_head] entry for `mm_struct`s
    [init_mm][init_mm]`.mmlist` which is protected by
    [mmlist_lock][mmlist_lock].

11. `start_code`, `end_code` - [Start, end) address for code section.

12. `start_data`, `end_data` - [Start, end) address for data section.

13. `start_brk`, `brk` - [Start, end) address of the heap.

14. `start_stack` - Surprisingly the address at the start of the stack region.

15. `arg_start`, `arg_end` - [Start, end) address for command-line arguments.

16. `env_start`, `env_end` - [Start, end] address for environment variables.

17. `rss` - Resident Set Size (RSS) is the number of resident pages for this
    process. The global zero page is not account for by RSS.

18. `total_vm` - The total memory space occupied by all VMA regions in this
    process.

19. `locked_vm` - The number of resident pages locked in memory.

20. `def_flags` - This has only one possible value if set - `VM_LOCKED` - and is
    used to determine all future mappings are locked by default.

21. `cpu_vm_mask` - A bitmask represents all possible CPUs in an SMP
    system. It's used by an [Inter-Processor Interrupt (IPI)][ipi] to determine
    if a processor should execute a particular function or not. This matters
    during TLB flush for each CPU.

22. `swap_address` - Used by the pageout daemon to record the last address that
    was swapped from when swapping out entire processes.

23. `dumpable` - Set via the [prctl()][prctl] userland function/system call -
    it's only meaningful when a process is being traced.

24. `context` - Architecture-specific MMU context.

* There are a number of functions which interact with `mm_struct`s:

1. [mm_init()][mm_init] - Initialises an `mm_struct` by setting starting values
   for each field, allocating a PGD, initialising spinlocks, etc.

2. [allocate_mm()][allocate_mm] - Allocates an `mm_struct` from the slab
   allocator (see chapter 8 for more on this.)

3. [mm_alloc()][mm_alloc] - Allocates an `mm_struct` using `allocate_mm()` and
   calls `mm_init()` to initialise it.

4. [exit_mmap()][exit_mmap] - Walks through an `mm_struct` and unmaps all VMAs
   associated with it.

5. [copy_mm()][copy_mm] - Makes an exact copy of the current tasks `mm_struct`
   needs for a new task. This is only used during `fork`.

6. [free_mm()][free_mm] - Returns the `mm_struct` to the slab allocator.

### 4.3.1 Allocating a Descriptor

* Two functions are available to allocate an `mm_struct` -
  [allocate_mm()][allocate_mm] which simply a pre-processor macro which
  allocates an `mm_Struct` from the slab allocator, whereas
  [mm_alloc()][mm_alloc] calls `allocate_mm()` and initialises it via
  [mm_init()][mm_init].

### 4.3.2 Initialising a Descriptor

* The first `mm_struct` in the system that is initialised is called
  [init_mm][init_mm]. All subsequent `mm_struct`s are copies of a parent
  `mm_struct`, meaning that `init_mm` has to be statically initialised at
  compile time.

* The static initialisation is performed by the macro [INIT_MM()][INIT_MM]:

```c
#define INIT_MM(name) \
{                                                       \
        mm_rb:          RB_ROOT,                        \
        pgd:            swapper_pg_dir,                 \
        mm_users:       ATOMIC_INIT(2),                 \
        mm_count:       ATOMIC_INIT(1),                 \
        mmap_sem:       __RWSEM_INITIALIZER(name.mmap_sem), \
        page_table_lock: SPIN_LOCK_UNLOCKED,            \
        mmlist:         LIST_HEAD_INIT(name.mmlist),    \
}
```

* After it is established, new `mm_struct`s are created using their parent
  `mm_struct` as a template via [copy_mm()][copy_mm] which uses
  [init_mm()][init_mm] to initialise process-specific fields.

### 4.3.3 Destroying a Descriptor

* When a new user increments the `mm_struct`'s usage count (i.e. `mm_users`),
  this is done with a simple atomic increment
  e.g. `atomic_inc(&mm->mm_users)`.

* However, when this field is decremented, this is done via [mmput()][mmput] so
  that if this field reaches zero, the appropriate data can be properly teared
  down.

* When `mm_users` reaches 0, the mapped regions are destroyed via
  [exit_mmap()][exit_mmap], then the page tables are destroyed because there are
  no longer any users of the userspace portions. The `mm_count` is decremented
  via [mmdrop()][mmdrop], because all users of the page tables and VMAs are
  counted as a single `mm_struct` user.

* As discussed above, when `mm_count` reaches 0, the `mm_struct will be
  destroyed.

[vmalloc]:http://fxr.watson.org/fxr/source/include/linux/vmalloc.h?v=linux-2.4.22#L37
[__vmalloc]:http://fxr.watson.org/fxr/source/mm/vmalloc.c?v=linux-2.4.22#L261
[PAGE_OFFSET]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L128
[__PAGE_OFFSET]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L81
[mem_map]:http://fxr.watson.org/fxr/source/mm/memory.c?v=linux-2.4.22#L73
[PKMAP_BASE]:http://fxr.watson.org/fxr/source/include/asm-i386/highmem.h?v=linux-2.4.22#L49
[kmap]:http://fxr.watson.org/fxr/source/include/asm-i386/highmem.h?v=linux-2.4.22#L62
[__kmap]:http://fxr.watson.org/fxr/source/include/asm-i386/highmem.h?v=linux-2.4.22#L65
[FIXADDR_START]:http://fxr.watson.org/fxr/source/include/asm-i386/fixmap.h?v=linux-2.4.22#L106
[FIXADDR_TOP]:http://fxr.watson.org/fxr/source/include/asm-i386/fixmap.h?v=linux-2.4.22#L104
[apic]:https://en.wikipedia.org/wiki/Advanced_Programmable_Interrupt_Controller
[__FIXADDR_SIZE]:http://fxr.watson.org/fxr/source/include/asm-i386/fixmap.h?v=linux-2.4.22#L105
[VMALLOC_RESERVE]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L129
[__VMALLOC_RESERVE]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L87
[mm_struct]:http://fxr.watson.org/fxr/source/include/linux/sched.h?v=linux-2.4.22#L206
[vm_area_struct]:http://fxr.watson.org/fxr/source/include/linux/mm.h?v=linux-2.4.22#L44

[fork]:http://man7.org/linux/man-pages/man2/fork.2.html
[clone]:http://man7.org/linux/man-pages/man2/clone.2.html
[mmap]:http://man7.org/linux/man-pages/man2/mmap.2.html
[mremap]:http://man7.org/linux/man-pages/man2/mremap.2.html
[munmap]:http://man7.org/linux/man-pages/man2/munmap.2.html
[shmat]:http://man7.org/linux/man-pages/man2/shmat.2.html
[shmdt]:http://man7.org/linux/man-pages/man2/shmdt.2.html
[execve]:http://man7.org/linux/man-pages/man2/execve.2.html
[exit]:http://man7.org/linux/man-pages/man2/exit.2.html
[cow]:https://en.wikipedia.org/wiki/Copy-on-write

[task_struct]:http://fxr.watson.org/fxr/source/include/linux/sched.h?v=linux-2.4.22#L283
[daemonize]:http://fxr.watson.org/fxr/source/kernel/sched.c?v=linux-2.4.22#L1326
[exit_mm]:http://fxr.watson.org/fxr/source/kernel/exit.c?v=linux-2.4.22#L322
[__exit_mm]:http://fxr.watson.org/fxr/source/kernel/exit.c?v=linux-2.4.22#L305
[linus-tlb]:http://lists-archives.com/linux-kernel/28329215-tlb-flush-multiple-pages-per-ipi-v5.html
[switch_mm]:http://fxr.watson.org/fxr/source/include/asm-i386/mmu_context.h?v=linux-2.4.22#L28
[enter_lazy_tlb]:http://fxr.watson.org/fxr/source/include/asm-i386/mmu_context.h?v=linux-2.4.22#L17
[start_lazy_tlb]:http://fxr.watson.org/fxr/source/kernel/exit.c?v=linux-2.4.22#L279
[swap_out]:http://fxr.watson.org/fxr/source/mm/vmscan.c?v=linux-2.4.22#L296
[exit_mmap]:http://fxr.watson.org/fxr/source/mm/mmap.c?v=linux-2.4.22#L1127
[rb-tree]:https://en.wikipedia.org/wiki/Red%E2%80%93black_tree
[find_vma]:http://fxr.watson.org/fxr/source/mm/mmap.c?v=linux-2.4.22#L661
[down_read]:http://fxr.watson.org/fxr/source/include/linux/rwsem.h?v=linux-2.4.22#L43
[down_write]:http://fxr.watson.org/fxr/source/include/linux/rwsem.h?v=linux-2.4.22#L65
[list_head]:http://fxr.watson.org/fxr/source/include/linux/list.h?v=linux-2.4.22#L18
[init_mm]:http://fxr.watson.org/fxr/source/arch/i386/kernel/init_task.c?v=linux-2.4.22#L12
[mmlist_lock]:http://fxr.watson.org/fxr/source/kernel/fork.c?v=linux-2.4.22#L224
[ipi]:https://en.wikipedia.org/wiki/Inter-processor_interrupt
[prctl]:http://man7.org/linux/man-pages/man2/prctl.2.html
[mm_init]:http://fxr.watson.org/fxr/source/kernel/fork.c?v=linux-2.4.22#L230
[allocate_mm]:http://fxr.watson.org/fxr/source/kernel/fork.c?v=linux-2.4.22#L227
[mm_alloc]:http://fxr.watson.org/fxr/source/kernel/fork.c?v=linux-2.4.22#L248
[copy_mm]:http://fxr.watson.org/fxr/source/kernel/fork.c?v=linux-2.4.22#L315
[free_mm]:http://fxr.watson.org/fxr/source/kernel/fork.c?v=linux-2.4.22#L228
[INIT_MM]:http://fxr.watson.org/fxr/source/include/linux/sched.h?v=linux-2.4.22#L238
[mmput]:http://fxr.watson.org/fxr/source/kernel/fork.c?v=linux-2.4.22#L276
[mmdrop]:http://fxr.watson.org/fxr/source/include/linux/sched.h?v=linux-2.4.22#L765
