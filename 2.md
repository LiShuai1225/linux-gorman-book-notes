# Chapter 2: Describing Physical Memory

* NUMA - Non-Uniform Memory Access. Memory arranged into banks, incurring a
  different cost for access depending on their distance from the processor.

* Each of these banks is called a 'node', represented by
  [struct pglist_data][pglist_data] _even if the arch is UMA_.

* The struct is always referenced by a typedef, `pg_data_t`.

* Every node is kept on a `NULL` terminated linked list, `pgdat_list`, and
  linked by `pg_data_t->node_next`.

* On UMA arches, only one `pg_data_t` structure called `contig_page_data` is
  used.

* Each node is divided into blocks called zones, which represent ranges of
  memory, described by [struct zone_struct][zone_struct], typedef-d to `zone_t`,
  one of `ZONE_DMA`, `ZONE_NORMAL` or `ZONE_HIGHMEM`.

* `ZONE_DMA` is kept within lower physical memory ranges that certain ISA
  devices need.

* `ZONE_NORMAL` memory is directly mapped into the upper region of the linear
  address space.

* `ZONE_HIGHMEM` is what's is left.

* In a 32-bit kernel the mappings are:

```
ZONE_DMA - First 16MiB of memory
ZONE_NORMAL - 16MiB - 896MiB
ZONE_HIGHMEM - 896 MiB - End
```

* Many kernel operations can _only_ take place in `ZONE_NORMAL`. So this is the
  most performance critical zone.

* Memory is divided into fixed-size chunks called _page frames_, represented by
  [struct page][page], and all of these are kept in a global `mem_map` array,
  usually stored at the beginning of `ZONE_NORMAL` or just after the area
  reserved for the loaded kernel image in low memory machines.

* Because the amount of memory directly accessible by the kernel (`ZONE_NORMAL`)
  is limited in size, Linux has the concept of _high memory_.

### 2.1 Nodes

* Each node is described by a `pg_data_t`, which is a `typedef` for
[struct pglist_data][pglist_data]:

```c
typedef struct pglist_data {
	zone_t node_zones[MAX_NR_ZONES];
	zonelist_t node_zonelists[GFP_ZONEMASK+1];
	int nr_zones;
	struct page *node_mem_map;
	unsigned long *valid_addr_bitmap;
	struct bootmem_data *bdata;
	unsigned long node_start_paddr;
	unsigned long node_start_mapnr;
	unsigned long node_size;
	int node_id;
	struct pglist_data *node_next;
 } pg_data_t;
```

### Fields

* `node_zones` - `ZONE_HIGHMEM`, `ZONE_NORMAL`, `ZONE_DMA`.

* `node_zonelists` - Order of zones allocations are preferred
  from. [build_zonelists()][build_zonelists] in `mm/page_alloc.c` sets up the
  order, called by [free_area_init_core()][free_area_init_core]. A failed
  allocation in `ZONE_HIGHMEM` may fall back to `ZONE_NORMAL` or back to
  `ZONE_DMA`.

* `nr_zones` - Number of zones in this node, between 1 and 3 (not all nodes will
  have all zones.)

* `node_mem_map` - First page of the [struct page][page] array that represents
  each physical frame in the node. Will be placed somewhere in the `mem_map` array.

* `valid_addr_bitmap` - Used by sparc/sparc64.

* `bdata` - Boot memory information only.

* `node_start_paddr` - Starting physical address of the node.

* `node_start_mapnr` - Page offset within global `mem_map`. Calculated in
  [free_area_init_core()][free_area_init_core] by determining number of pages
  between `mem_map` and local `mem_map` for this node called `lmem_map`.

* `node_size` - Total number of pages in this zone.

* `node_id` - Node ID (NID) of node, starting at 0.

* `node_next` - Pointer to next node, `NULL` terminated list.

* Nodes are maintained on a list called `pgdat_list`. Nodes are placed on the
  list as they are initialised by the [init_bootmem_core()][init_bootmem_core]
  function. They can be iterated over using [for_each_pgdat][for_each_pgdat], e.g.:

```c
pg_data_t *pgdat;

for_each_pgdat(pgdat)
	pr_debug("node %d: size=%d", pgdat->node_id, pgdat->node_size);
```

## 2.2 Zones

* Each zone is described by a [struct zone_struct][zone_struct]:

```c
typedef struct zone_struct {
	/*
	 * Commonly accessed fields:
	 */
	spinlock_t              lock;
	unsigned long           free_pages;
	unsigned long           pages_min, pages_low, pages_high;
	int                     need_balance;

	/*
	 * free areas of different sizes
	 */
	free_area_t             free_area[MAX_ORDER];

	/*
	 * wait_table           -- the array holding the hash table
	 * wait_table_size      -- the size of the hash table array
	 * wait_table_shift     -- wait_table_size
	 *                              == BITS_PER_LONG (1 << wait_table_bits)
	 *
	 * The purpose of all these is to keep track of the people
	 * waiting for a page to become available and make them
	 * runnable again when possible. The trouble is that this
	 * consumes a lot of space, especially when so few things
	 * wait on pages at a given time. So instead of using
	 * per-page waitqueues, we use a waitqueue hash table.
	 *
	 * The bucket discipline is to sleep on the same queue when
	 * colliding and wake all in that wait queue when removing.
	 * When something wakes, it must check to be sure its page is
	 * truly available, a la thundering herd. The cost of a
	 * collision is great, but given the expected load of the
	 * table, they should be so rare as to be outweighed by the
	 * benefits from the saved space.
	 *
	 * __wait_on_page() and unlock_page() in mm/filemap.c, are the
	 * primary users of these fields, and in mm/page_alloc.c
	 * free_area_init_core() performs the initialization of them.
	 */
	wait_queue_head_t       * wait_table;
	unsigned long           wait_table_size;
	unsigned long           wait_table_shift;

	/*
	 * Discontig memory support fields.
	 */
	struct pglist_data      *zone_pgdat;
	struct page             *zone_mem_map;
	unsigned long           zone_start_paddr;
	unsigned long           zone_start_mapnr;

	/*
	 * rarely used fields:
	 */
	char                    *name;
	unsigned long           size;
 } zone_t;
```

* `lock` - Spinlock protects the zone from concurrent accesses.

* `free_pages` - Total number of free pages in the zone.

* `pages_min`, `pages_low`, `pages_high` - Watermarks - If `free_pages <
  pages_low`, `kswapd` is woken up and swaps pages out asynchronously. If the
  page consumption doesn't slow down fast enough from this, `kswapd` switches
  into a mode where pages are freed synchronously in order to return the system
  to health.

* `need_balance` - This indicates to `kswapd` that it needs to balance the zone,
  i.e. `free_pages` has hit one of the watermarks.

* `free_area` - Free area bitmaps used by the buddy allocator.

* `wait_table` - Hash table of wait queues of processes waiting on a page to be
  freed. This is meaningful to [wait_on_page()][wait_on_page] and
  [unlock_page()][unlock_page]. A 'wait table' is used because, if processes all
  waited on a single queue, there'd be a big race between processes for pages
  which are locked on wake up (known as a 'thundering herd'.)

* `wait_table_size` - Number of queues in the hash table (power of 2.)

* `wait_table_shift` - Number of bits in a long - binary logarithm of `wait_table_size`.

* `zone_pgdat` - Points to the parent `pg_data_t`.

* `zone_mem_map` - First page in a global `mem_map` that this zone refers to.

* `zone_start_paddr` - Starting physical address of the zone.

* `zone_start_mapnr` - Page offset within global `mem_map`.

* `name` - String name of the zone - `"DMA"`, `"Normal"` or `"HighMem"`.

* `size` - Size of zone in pages.

## 2.2.1 Zone Watermarks

[pglist_data]:http://fxr.watson.org/fxr/source/include/linux/mmzone.h?v=linux-2.4.22#L129
[build_zonelists]:http://fxr.watson.org/fxr/source/mm/page_alloc.c?v=linux-2.4.22#L589
[free_area_init_core]:http://fxr.watson.org/fxr/source/mm/page_alloc.c?v=linux-2.4.22#L684
[page]:http://fxr.watson.org/fxr/source/include/linux/mm.h?v=linux-2.4.22#L154
[for_each_pgdat]:http://fxr.watson.org/fxr/source/include/linux/mmzone.h?v=linux-2.4.22#L172
[zone_struct]:http://fxr.watson.org/fxr/source/include/linux/mmzone.h?v=linux-2.4.22#L37
[wait_on_page]:http://fxr.watson.org/fxr/source/include/linux/pagemap.h?v=linux-2.4.22#L94
[unlock_page]:http://fxr.watson.org/fxr/source/mm/filemap.c?v=linux-2.4.22#L874
