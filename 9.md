# Chapter 9: High Memory Management

* The kernel can only directly address memory for which it has set up a page
  table entry. In the common case, the user/kernel address space split of
  3GiB/1GiB implies that, at best, only 896MiB of memory may be directly
  accessed at any given time on a 32-bit machine (see 4.1)

* On 64-bit hardware this is not an issue as there are vast amounts of virtual
  address space, however 32-bit is another story.

* On 32-bit hardware Linux temporarily maps pages from high memory into the
  lower page tables (discussed further in 9.2.)

* In I/O operations, not all devices are able to address high memory or even all
  the memory available to the CPU (consider [PAE][pae].) Asking the device to
  write to memory will fail at best and fuck the kernel at worst. To work around
  this issue we can use a 'bounce buffer' which is discussed in 9.5.

## 9.1 Managing the PKMap Address Space

* The 'Persistent Kernel Map' (PKMap) address space is reserved at the top of
  the kernel page tables from [PKMAP_BASE][PKMAP_BASE] to
  [FIXADDR_START][FIXADDR_START].

* On i386, `PKMAP_BASE` is `0xfe000000`, and `FIXADDR_START` varies depending on
  configuration options, but is typically only a few pages from the end of the
  linear address space - this leaves us around 32MiB of page table space for
  mapping pages from high memory into usable space.

* For mapping pages, a single page set of PTEs is stored at the beginning of the
  PKMap area to allow 1,024 high pages to be mapped into low memory for short
  periods via [kmap()][kmap] and to be unmapped with [kunmap()][kunmap].

* The pool of 1,024 pages (4MiB) seems very small, however pages mapped by
  `kmap()` are only mapped for a _very_ short period of time.

* Comments in the code suggest that there was a plan to allocate contiguous page
  table entries to expand this area however this hasn't been performed in
  2.4.22, so a large portion of the PKMap is unused.

* The page table entry for use with `kmap()` is
  [pkmap_page_table][pkmap_page_table] and is located at
  [PKMAP_BASE][PKMAP_BASE] which is set up during system initialisation (on i386
  this takes place at the end of [pagetable_init()][pagetable_init].

* The pages for the PGD and PMD entries are allocated by the boot memory
  allocator to ensure they exist.

* The state of these page table entries is managed by
  [pkmap_count][pkmap_count], which has [LAST_PKMAP][LAST_PKMAP] entries in it -
  on i386 without PAE this is 1,024 and with PAE it is 512. More accurately,
  though the code doesn't make it clear, `LAST_PKMAP` is equivalent to
  [PTRS_PER_PTE][PTRS_PER_PTE/3lvl].

* Each element in `pkmap_count` is not exactly a reference count, but is very
  close - if the entry is 0, the page is free and has not been used since the
  last TLB flush. If it's 1, the slot is unused, but a page is still mapped
  there waiting for a TLB flush, and finally if it's any higher it has `n - 1`
  users, taking `n` to be the element value.

* Flushes are delayed until every slot has been used at least once because a
  global flush is required for all CPUs when the global page tables are modified
  and this is _extremely_ expensive.

## 9.2 Mapping High Memory Pages

* Let's take a look at the API for mapping pages from high memory:

1. [kmap()][kmap] - Takes a [struct page][page] from high memory and maps it
   into low memory, returning a virtual address of the mapping.

2. [kmap_nonblock()][kmap_nonblock] - Same as `kmap()` only it won't block if
   slots are not available and instead returns `NULL`. This isn't the same as
   `kmap_atomic()` which uses specially reserved slots.

3. [kmap_atomic()][kmap_atomic] - This uses slots maintained in the map for
   atomic use by interrupts (see 9.4 for more details), however their use is
   _highly_ discouraged and callers may not sleep or schedule. You _really_ have
   to require atomic high memory mapping to use this.

* The kmap pool is rather small so it's vital that callers call
  [kunmap()][kunmap] as quickly as possible, as the pressure on this small
  window grows incrementally worse as the size of high memory grows in
  comparison to low memory.

* `kmap()` itself is fairly simple:

1. Check to ensure an interrupt is not calling the function (as it may sleep),
   calling [out_of_line_bug()][out_of_line_bug] if so - we call this rather than
   [BUG()][BUG] because the latter would panic the system if called from an
   interrupt handler (LS - it seems `out_of_line_bug()` calls `BUG()` though??)

2. Check that the page is below [highmem_start_page][highmem_start_page],
   because pages below this level are already visible and do not need to be
   mapped.

3. Check whether the page is already in low memory, if so simply return the
   address. This step is important - it means users can use it unconditionally
   knowing that if it is already a low memory page the function can still be
   used safely.

4. Finally call [kmap_high()][kmap_high] to do the real work.

* `kmap_high()` (and subsequently [map_new_virtual()][map_new_virtual] does the
  following:

1. Check the `page->virtual` field, which is set if the page is already
   mapped. If it's `NULL`, [map_new_virtual()][map_new_virtual] provides a
   mapping for the page.

2. `map_new_virtual()` simply linearly scans [pkmap_count][pkmap_count],
   starting at [last_pkmap_nr][last_pkmap_nr] to avoid searching the same areas
   repeatedly between `kmap()`s.

3. When `last_pkmap_nr` wraps around to 0,
   [flush_all_zero_pkmaps()][flush_all_zero_pkmaps] sets all entries from 1 to 0
   before flushing the TLB.

4. If, after another scan, an entry is _still_ not found, the process sleeps on
   the [pkmap_map_wait][pkmap_map_wait] wait queue until it is woken up after
   the next [kunmap()][kunmap].

5. After a mapping has been created, the corresponding entry in the
   `pkmap_count` array is incremented, and the virtual address in low memory is
   returned.

## 9.3 Unmapping Pages

* Taking a look at the API for unmapping pages from high memory:

1. [kunmap()][kunmap] - Unmaps a [struct page][page] from low memory and frees
   up the page table entry that was mapping it.

2. [kunmap_atomic()][kunmap_atomic] - Unamps a page that was mapped atomically
   via [kmap_atomic()][kmap_atomic].

* `kunmap()`, similar to [kmap()][kmap], checks that the caller is not from
  interrupt context and that the memory is below
  [highmem_start_page][highmem_start_page], before calling
  [kunmap_high()][kunmap_high] to do the real work of unmapping the page.

* `kunmap_high()` decrements the corresponding element for the page in
  [pkmap_count][pkmap_count]. If it reaches 1 (no users but a TLB flush
  required), any process waiting on the [pkmap_map_wait][pkmap_map_wait] wait
  queue is woken up because a slot is now available.

* `kunmap_high()` does not unmap the page from the page tables because that
  would require an expensive TLB flush, this is delayed until
  [flush_all_zero_pkmaps()][flush_all_zero_pkmaps] is called.

[pae]:https://en.wikipedia.org/wiki/Physical_Address_Extension

[PKMAP_BASE]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/asm-i386/highmem.h#L49
[FIXADDR_START]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/asm-i386/fixmap.h#L106
[kmap]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/asm-i386/highmem.h#L62
[kunmap]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/asm-i386/highmem.h#L74
[pkmap_page_table]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/mm/highmem.c#L38
[pagetable_init]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/arch/i386/mm/init.c#L205
[pkmap_count]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/mm/highmem.c#L33
[LAST_PKMAP]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/asm-i386/highmem.h#L51
[PTRS_PER_PTE/3lvl]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/asm-i386/pgtable-3level.h#L27

[kmap_nonblock]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/asm-i386/highmem.h#L63
[kmap_atomic]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/asm-i386/highmem.h#L83
[page]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/linux/mm.h#L154
[out_of_line_bug]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/linux/kernel.h#L175
[BUG]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/asm-i386/page.h#L91
[highmem_start_page]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/mm/memory.c#L57
[kmap_high]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/mm/highmem.c#L132
[map_new_virtual]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/mm/highmem.c#L80
[last_pkmap_nr]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/mm/highmem.c#L34
[flush_all_zero_pkmaps]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/mm/highmem.c#L42
[pkmap_map_wait]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/mm/highmem.c#L40

[kunmap_atomic]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/include/asm-i386/highmem.h#L109
[kunmap_high]:https://github.com/lorenzo-stoakes/linux-historical/blob/v2.4.22/mm/highmem.c#L157
