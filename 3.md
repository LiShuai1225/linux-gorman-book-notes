# Chapter 3: Page Table Management

## Overview

In linux 2.4.22 a 32-bit virtual address actually consists of 4 offsets,
indexing into the PGD, PMD, PTE and the address's actual physical page of
memory (the PGD, PMD and PTE are defined below.)

Each process has a known PGD address. Subsequently (noting these are all
_physical_ addresses):

```
pmd_table = pgd_table[pgd_offset] & PAGE_MASK
pte_table = pmd_table[pmd_offset] & PAGE_MASK
phys_page = pte_table[pte_offset] & PAGE_MASK
phys_addr = phys_page[page_offset]
```

__NOTE:__ `PAGE_MASK = ~(1<<12 - 1)` excludes the least significant bits since
pages are page-aligned.

This indirection exists in order that each process need only take up 1 physical
page of memory for its overall page directory - i.e. entries can be empty and
are filled as needed (the layout is 'sparse'.)

A virtual address therefore looks like:

```
<- PGD bits -><- PMD bits -><- PTE bits -><- Offset bits ->
[ PGD Offset ][ PMD Offset ][ PTE Offset ][  Page Offset  ]
```

As discussed below, it's possible for offsets to be of 0 bits length, meaning
the directory in question is folded back onto the nearest higher directory.

In 32-bit non-[PAE][PAE] we have:

```
<-  10 bits -><-  0 bits  -><- 10 bits  -><-   12 bits   ->
[ PGD Offset ][ PMD Offset ][ PTE Offset ][  Page Offset  ]
```

In 32-bit [PAE][PAE] we have:

```
<-  2 bits  -><-  9 bits  -><-  9 bits  -><-   12 bits   ->
[ PGD Offset ][ PMD Offset ][ PTE Offset ][  Page Offset  ]
```

Importantly here all physical pages are stored as 64-bit values. A 32-bit intel
PAE configuration could physically address 36 bits (64GB) so only 4 bits of the
higher order byte are used for addressing (some of the other bits are used for
flags, see the [PAE wikipedia article][PAE] for more details.)

## Introduction

* Linux has a unique means of abstracting the architecture-specific details of
  physical pages. It maintains a concept of a three-level page table (note: this
  is relevant to 2.4.22 of course, in later kernels it is [4 levels][4level].)
  The 3 levels are:

1. Page Global Directory (PGD)

2. Page Middle Directory (PMD)

3. Page Table Entry (PTE)

* This three-level page table structure is implemented _even if_ the underlying
  architecture does not support it.

* Though this is conceptually easy to understand, it means the distinction
  between different types of pages is really blurry, and page types are
  identified by their flags or what lists they reside in rather than the objects
  they belong to.

* Architectures that manage their [Memory Management Unit (MMU)][mmu]
  differently are expected to emulate the three-level page tables. E.g. on x86
  without [PAE][PAE], only two page table levels are available, so the PMD is
  defined to be of size 1 (2^0) and 'folds back' directly onto the PGD, and this
  is optimised out at compile time.

* For architectures that do not manage their cache or
  [Translation lookaside buffer (TLB)][tlb] automatically,
  architecture-dependent hooks have to be left in the code for when the TLB and
  CPU caches need to be altered and flushed, even if they are null operations on
  some architectures like the x86 (discussed further in section 3.8.)

* Virtual memory is divided into separate directories so we can have a 'sparse'
  set of data structures for each process. Each PGD takes up a page of memory
  (4KiB on i386), rather than the 1MiB it would take to map the whole 4GiB
  address space if it were only a single list.

## 3.1 Describing the Page Directory

* Each process has a pointer [mm_struct][mm_struct]`->pgd` to its own PGD, which
  is a physical page frame containing an array of type [pgd_t][pgd_t], an
  architecture specific type defined in `asm/page.h`:

```c
struct mm_struct {
	struct vm_area_struct * mmap;		/* list of VMAs */
	rb_root_t mm_rb;
	struct vm_area_struct * mmap_cache;     /* last find_vma result */
	pgd_t * pgd;
	atomic_t mm_users;			/* How many users with user space? */
	atomic_t mm_count;			/* How many references to "struct mm_struct" (users count as 1) */
	int map_count;				/* number of VMAs */
	struct rw_semaphore mmap_sem;
	spinlock_t page_table_lock;		/* Protects task page tables and mm->rss */

	struct list_head mmlist;		/* List of all active mm's.  These are globally strung
						 * together off init_mm.mmlist, and are protected
						 * by mmlist_lock
						 */

	unsigned long start_code, end_code, start_data, end_data;
	unsigned long start_brk, brk, start_stack;
	unsigned long arg_start, arg_end, env_start, env_end;
	unsigned long rss, total_vm, locked_vm;
	unsigned long def_flags;
	unsigned long cpu_vm_mask;
	unsigned long swap_address;

	unsigned dumpable:1;

	/* Architecture-specific MM context */
	mm_context_t context;
};
```

* Page tables are loaded differently depending on architecture. On x86, process
  page table is loaded by copying `mm_struct->pgd` into the `cr3` register,
  which has the side effect of flushing the [TLB][tlb].

* In fact, on x86, [__flush_tlb()][__flush_tlb] is implemented by copying `cr3`
  into a temporary register, then copying that result back in again.

* Each _active_ entry in the PGD table points to a page frame (i.e. physical
  page of memory) containing an array of PMD entries of type [pmd_t][pmd_t],
  which in term point to page frames containing PTEs of type [pte_t][pte_t],
  which finally point to page frames containing the actual user data.

* The following code defines how we can determine a physical address manually
  (of course in the majority of cases access is performed transparently by the
  MMU):

```c
/* Shared between 2-level and 3-level i386: */

#define pgd_offset(mm, address) ((mm)->pgd+pgd_index(address))
#define pgd_index(address) ((address >> PGDIR_SHIFT) & (PTRS_PER_PGD-1))
#define pgd_page(pgd) \
	((unsigned long) __va(pgd_val(pgd) & PAGE_MASK))
#define __va(x) ((void *)((unsigned long)(x)+PAGE_OFFSET))
/*
 * Note that pgd_t is a struct containing just the pgd pointer, used to take
 * advantage of C type checking.
 */
#define pgd_val(x) ((x).pgd)

#define PAGE_SHIFT   12 /* 4 KiB. */
#define PAGE_SIZE    (1UL << PAGE_SHIFT)
#define PAGE_MASK    (~(PAGE_SIZE-1))

#define __pte_offset(address) \
		((address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
#define pte_offset(dir, address) ((pte_t *) pmd_page(*(dir)) + \
			__pte_offset(address))

#define pmd_page(pmd) \
	((unsigned long) __va(pmd_val(pmd) & PAGE_MASK))

/* In 2-level (non-PAE) configuration: */

#define PGDIR_SHIFT  22
#define PTRS_PER_PGD 1024

#define PMD_SHIFT    22
#define PTRS_PER_PMD 1

#define PTRS_PER_PTE 1024

static inline pmd_t * pmd_offset(pgd_t * dir, unsigned long address)
{
	return (pmd_t *) dir;
}

/* In 3-level (PAE) configuration: */

#define PGDIR_SHIFT  30
#define PTRS_PER_PGD 4

#define PMD_SHIFT    21
#define PTRS_PER_PMD 512

#define PTRS_PER_PTE 512

#define pmd_offset(dir, address) ((pmd_t *) pgd_page(*(dir)) + \
		__pmd_offset(address))
#define __pmd_offset(address) \
		(((address) >> PMD_SHIFT) & (PTRS_PER_PMD-1))
```

* [pgd_offset()][pgd_offset] Determines the _physical address_ of the PGD entry
  for a given virtual address given that address and the process's
  [mm_struct][mm_struct], via [pgd_index()][pgd_index]. We shift the address
  right by [PGDIR_SHIFT][PGDIR_SHIFT/2lvl] bits, before limiting to the number
  of possible pointers, [PTRS_PER_PGD][PTRS_PER_PGD/2lvl]. This number will be a
  power of 2, so the number subtracted by 1 is a mask of all possible indexes
  into the PGD, e.g. `1000` allows all permutations of `[0000,0111]`. Finally we
  add the known physical address of the pgd from `mm->pgd`.

### 2-level i386

* The structure is a PGD table with 1024 entries -> PTE table with 1024
  entries. The PMD is just an alias for the PGD.

* In a 2-level i386 configuration, [PGDIR_SHIFT][PGDIR_SHIFT/2lvl] is 22 and
  [PTRS_PER_PGD][PTRS_PER_PGD/2lvl] is 1024 so we use the upper 10 bits to
  offset into 1024 entries. [pmd_offset][pmd_offset/2lvl] simply returns a
  (physical) pointer to the PGD entry, so when we translate to PMD, we are just
  looking at the same PGD entry again. The compiler will remove this unnecessary
  intermediate step, but we retain the same abstraction in generic code. Very
  clever!

* Note that the input `dir` value is a _virtual_ address of type `pgd_t *`,
  whose value is a _physical_ address for a PGD _entry_, so the returned `pmd_t
  *` is exactly the same - a virtual address which contains a physical address
  value.

* Now we have the physical address of our 'PMD' entry (i.e. the same PGD entry),
  we can use this to determine our PTE and finally get hold of the address of
  the physical page we want, exactly the same as we would with a 3-level system.

### 3-level i386 (PAE)

* The structure is a PGD table with only 4 entries -> PMD table with 512 64-bit
  entries (to store more address bits) -> PTE table with 512 entries.

* In a 3-level i386 PAE configuration, [PGDIR_SHIFT][PGDIR_SHIFT/3lvl] is 30 and
  [PTRS_PER_PGD][PTRS_PER_PGD/3lvl] is 4 and we have an actually real PMD :) to
  get it, we use [pmd_offset()][pmd_offset/3lvl], which starts by calling
  [pgd_page()][pgd_page] to get the _virtual_ address of the physical PMD
  contained inside the PGD entry. It starts by calling [pgd_val()][pgd_val]:

* `pgd_val()` gets the physical address of the specified PGD _entry_ (note that
  the `dir` argument will be an entry in the `PGD`, rather than the PGD table as
  a whole. This confused me for a while :) - [pgd_t][pgd_t] is actually a
  `struct` containing a pointer to make use of C type checking, so it does this
  by simply accesses the value member.

* Now we come back to `pgd_page()` which uses [PAGE_MASK][PAGE_MASK] (this masks
  out the lower 12 bits, i.e. 4KiB's worth) to get the physical page of the PMD,
  ignoring any flags contained in the PGD entry, then uses [__va][__va] to
  convert this physical _kernel_ address into a virtual _kernel_ address,
  i.e. simply offsetting by [__PAGE_OFFSET][__PAGE_OFFSET] (kernel addresses are
  mapped directly like this), we need the _virtual_ address as we're returning a
  `pmd_t *`, not a `pmd_t`.

* Finally now we have a virtual address for the start of the PMD entry table,
  `pmd_offset()` needs to get the offset of the specific `pmd_t` we're after in
  order to return a `pmd_t *` virtual address to it. It does this via
  [__pmd_offset][__pmd_offset] which does the same job as
  [pgd_index()][pgd_index], only using [PMD_SHIFT][PMD_SHIFT/3lvl] (21) and
  [PTRS_PER_PMD][PTRS_PER_PMD/3lvl] (512.) It is a 21-bit shift because we've
  used 2 bits for the PMD entry, leaving us with 19 to address 512 entries.

### Looking up the PTE

* Similar to [pmd_offset()][pmd_offset/3lvl], [pte_offset()][pte_offset] grabs
  the contents of the PMD entry via [pmd_page()][pmd_page] (in a 2-level system
  this will just be the same as [pgd_page()][pgd_page]), using
  [pmd_val()][pmd_val] to grab the `pmd` field from the [pmd_t][pmd_t] and
  finally return a virtual `pte_t *` value. Finally, it returns the _physical_
  address of the PTE as a [pte_t *][pte_t].

* In a 3-level ([PAE][PAE]) system, the [pte_t][pte_t] is a 64-bit value, meaning
  [pte_val()][pte_val/3lvl] returns a 64-bit physical address.

[4level]:https://lwn.net/Articles/117749/
[mmu]:https://en.wikipedia.org/wiki/Memory_management_unit
[tlb]:https://en.wikipedia.org/wiki/Translation_lookaside_buffer
[mm_struct]:http://fxr.watson.org/fxr/source/include/linux/sched.h?v=linux-2.4.22#L206
[pgd_t]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22;#L42
[__flush_tlb]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L38
[pmd_t]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L41
[pte_t]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L40
[pgd_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L331
[pgd_index]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L327
[PGDIR_SHIFT/2lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L8
[PTRS_PER_PGD/2lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L9
[PGDIR_SHIFT/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L14
[PTRS_PER_PGD/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L15
[pmd_offset/2lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L55
[pmd_offset/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L72
[pgd_page]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L52
[PAGE_MASK]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L7
[pgd_val]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L55
[__va]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L133
[PAE]:https://en.wikipedia.org/wiki/Physical_Address_Extension
[__pmd_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L336
[PMD_SHIFT/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L21
[PTRS_PER_PMD/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L22
[pte_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L342
[__pte_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L340
[pmd_page]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L323
[pte_val/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L43
[pmd_val]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L54
[__PAGE_OFFSET]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L81
