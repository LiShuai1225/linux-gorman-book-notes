# Chapter 3: Page Table Management

## (LS) Overview

In linux 2.4.22 a 32-bit virtual address actually consists of 4 offsets,
indexing into the PGD, PMD, PTE and the address's actual physical page of
memory (the PGD, PMD and PTE are defined below.)

Each process has a known PGD address. Subsequently (noting these are all
_physical_ addresses):

```
pmd_table = pgd_table[pgd_offset] & PAGE_MASK
pte_table = pmd_table[pmd_offset] & PAGE_MASK
phys_page = pte_table[pte_offset] & PAGE_MASK
phys_addr = phys_page[page_offset]
```

__NOTE:__ This is pseudo-code.

__NOTE:__ `PAGE_MASK = ~(1<<12 - 1)` excludes the least significant bits since
pages are page-aligned.

This indirection exists in order that each process need only take up 1 physical
page of memory for its overall page directory - i.e. entries can be empty and
are filled as needed (the layout is 'sparse'.)

A virtual address therefore looks like:

```
<-                    BITS_PER_LONG                      ->
<- PGD bits -><- PMD bits -><- PTE bits -><- Offset bits ->
[ PGD Offset ][ PMD Offset ][ PTE Offset ][  Page Offset  ]
                                          <----- PAGE_SHIFT
                            <-------------------- PMD_SHIFT
              <-------------------------------- PGDIR_SHIFT
```

As discussed below, it's possible for offsets to be of 0 bits length, meaning
the directory in question is folded back onto the nearest higher directory.

In 32-bit non-[PAE][PAE] we have:

```
<-  10 bits -><-  0 bits  -><- 10 bits  -><-   12 bits   ->
[ PGD Offset ][ PMD Offset ][ PTE Offset ][  Page Offset  ]
```

In 32-bit [PAE][PAE] we have:

```
<-  2 bits  -><-  9 bits  -><-  9 bits  -><-   12 bits   ->
[ PGD Offset ][ PMD Offset ][ PTE Offset ][  Page Offset  ]
```

Importantly here all physical pages are stored as 64-bit values. A 32-bit intel
PAE configuration could physically address 36 bits (64GB) so only 4 bits of the
higher order byte are used for addressing (some of the other bits are used for
flags, see the [PAE wikipedia article][PAE] for more details.)

## (LS) Page Table Function Cheat Sheet

* `pgd_offset(mm, address)` - Given a process's [struct mm_struct][mm_struct]
  (e.g. for the current process this would be `current->mm`) and a _virtual_
  `address`, returns the _virtual address_ of the corresponding PGD
  _entry_ (i.e. a `pgd_t *`.)

* `pmd_offset(dir, address)` - Given `dir`, a _virtual_ address of type `pgd_t
  *`, i.e. a pointer to the _entry_ which contains the PMD table's _physical_
  address, and a _virtual_ `address`, returns the virtual address of the
  corresponding PMD entry of type `pmd_t *`.

* `pte_offset(dir, address)` - Given `dir`, a _virtual_ address of type `pmd_t
  *`, i.e. a pointer to the _entry_ which contains the PTE table's _physical_
  address, and a _virtual_ `address`, returns the virtual address of the
  corresponding PTE entry of type `pte_t *`.

* `__[pgd,pmd,pte]_offset(address)` - Given a _virtual_ `address`, returns the
  index of the `[pgd,pmd,pte]_t` entry for this address within the PGD, PMD,
  PTE.

* `pgd_index(address)` - Given the _virtual_ `address`, [pgd_index()][pgd_index]
  returns the index of the [pgd_t][pgd_t] entry for this address within the
  PGD. Equivalent to `__pgd_offset(address)`.

* `[pgd,pmd]_page(entry)` - Given `entry`, a page table entry containing a
  _physical_ address of type `[pgd,pmd]_t`, returns the _virtual_ address of the
  page referenced by this entry. So [pgd_page()][pgd_page] returns `pmd_t
  *pmd_table` and [pmd_page()][pmd_page] returns `pte_t *pte_table`.

* `pte_page(entry)` - Given `entry`, a page table entry containing a _physical_
  address of type `pte_t`, [pte_page()][pte_page] returns the [struct
  page][page] entry (also typedef'd to `mem_map_t`) for the referenced page.

* `[pgd,pmd,pte]_val(entry)` - Given `entry`, a page table entry containing a
  _physical_ address of type `[pgd,pmd,pte]_t`, returns the absolute value of
  the physical address (i.e. `unsigned long` or `unsigned long long`.) Note that
  when using [PAE][PAE] this will be a 64-bit value.

* `[pgd,pmd,pte]_none(entry)` - Given `entry`, a page table entry containing a
  _physical_ address of type `[pgd,pmd,pte]_t`, returns true if it does not
  exist, e.g. [pmd_none()][pmd_none].

* `[pgd,pmd,pte]_present(entry)` - Given `entry`, a page table entry containing
  a _physical_ address of type `[pgd,pmd,pte]_t`, returns true if it has the
  present bit set e.g. [pmd_present()][pmd_present].

* `[pgd,pmd,pte]_clear(entry)` - Given `entry`, a page table entry containing a
  _physical_ address of type `[pgd,pmd,pte]_t`, clears the entry e.g.
  [pmd_clear()][pmd_clear].

* `[pgd,pmd]_bad(entry)` - Given `entry`, a page table entry containing a
  _physical_ address of type `[pgd,pmd]_t`, returns true if the entry is not in
  a state where the contents can be safely modified, e.g. [pmd_bad()][pmd_bad].

* `pte_dirty(entry)` - Given `entry`, a page table entry containing a _physical_
  address of type `pte_t`, returns true if the PTE has the dirty bit set,
  e.g. [pte_dirty()][pte_dirty]. __Important:__ [pte_present()][pte_present]
  __must__ return true for this to be useable.

* `pte_mkdirty(entry)`, `pte_mkclean(entry)` - Given `entry`, a page table entry
  containing a _physical_ address of type `pte_t`, sets or clears the dirty bit
  respectively - [pte_mkdirty()][pte_mkdirty], [pte_mkclean()][pte_mkclean].

* `pte_young(entry)` - Given `entry`, a page table entry containing a _physical_
  address of type `pte_t`, returns true if the PTE has the accessed flag set (on
  x86), e.g. [pte_young()][pte_young]. __Important:__
  [pte_present()][pte_present] __must__ return true for this to be useable.

* `pte_mkyoung(entry)`, `pte_mkold(entry)` - Given `entry`, a page table entry
  containing a _physical_ address of type `pte_t`, sets or clears the dirty bit
  respectively - [pte_mkyoung()][pte_mkyoung], [pte_mkold()][pte_mkold].

* `pte_read(entry)` - Given `entry`, a page table entry containing a _physical_
  address of type `pte_t`, returns true if the PTE is readable, which is
  implemented by the user flag on x86, e.g. [pte_read()][pte_read].

* `pte_mkread(entry)`, `pte_rdprotect(entry)` - Given `entry`, a page table
  entry containing a _physical_ address of type `pte_t`, sets or clears the user
  flag (on x86) respectively - [pte_mkread()][pte_mkread],
  [pte_rdprotect()][pte_rdprotect].

* `pte_write(entry)` - Given `entry`, a page table entry containing a _physical_
  address of type `pte_t`, returns true if the PTE is writeable,
  e.g. [pte_write()][pte_write].

* `pte_mkwrite(entry)`, `pte_wrprotect(entry)` - Given `entry`, a page table
  entry containing a _physical_ address of type `pte_t`, sets or clears the R/W
  flag respectively - [pte_mkwrite()][pte_mkwrite],
  [pte_wrprotect()][pte_wrprotect].

* `pte_exec(entry)` - Given `entry`, a page table entry containing a _physical_
  address of type `pte_t`, returns true if the PTE is executable. Note that on
  x86 at the time of 2.4.22 there was no [NX bit][nx-bit], in fact it was only
  added circa pentium 4, and only in 32-bit mode if [PAE][PAE] is enabled, as it
  is set at the 63rd (most significant) bit. In i386, it's equivalent to
  [pte_read()][pte_read] and checks for the user bit.

* `pte_mkexec(entry)`, `pte_exprotect(entry)` - Given `entry`, a page table
  entry containing a _physical_ address of the type `pte_t`, sets or clears the
  exec bit (or rather user bit in i386) respectively -
  [pte_mkexec()][pte_mkexec], [pte_exprotect()][pte_exprotect].

* `mk_pte(page, pgprot)` - Given `page`, a [struct page][page] and `pgprot`, a
  set of protection bits of type [pgprot_t][pgprot_t], returns a `pte_t` ready
  to be inserted into the page table. This macro in turn calls
  [__mk_pte][__mk_pte/3lvl] which uses a page number (i.e. the address shifted
  to the right by [PAGE_SHIFT][PAGE_SHIFT] bits) to determine how to populate
  the `pte_t` - [mk_pte()][mk_pte].

* `mk_pte_phys(physpage, pgprot)` - Given `physpage`, a physical address and
  `pgprot`, a set of protection bits of type `pgprot_t`, returns a `pte_t` ready
  to be inserted into the page table, similar to `mk_pte()` -
  [mk_pte_phys()][mk_pte_phys].

* `set_pte(ptep, pte)` - Given `ptep`, a pointer to a `pte_t` in the page table,
  and a PTE entry of type `pte_t`, this function sets it in place. This is a
  function rather than simply `*ptep = ptep;` because in some architectures
  (including PAE x86) additional work needs to be done. This is where the PTE
  returned by [mk_pte()][mk_pte] or [mk_pte_phys()][mk_pte_phys] is assigned to
  the actual page table - [set_pte()][set_pte/2lvl] for 2-level (non-PAE) x86,
  [set_pte()][set_pte/3lvl] for 3-level ([PAE][PAE]) x86.

* `pte_clear(xp)` - Given `xp`, a pointer to a `pte_t` in the page table,
  clears the entry altogether - [pte_clear()][pte_clear].

* `ptep_get_and_clear(ptep)` - Given `ptep`, a pointer to a `pte_t` in the page
  table, returns the existing PTE entry and clears it - this is useful when
  modification needs to be made to either the PTE protection or the
  [struct page][page] itself - [pte_get_and_clear()][pte_get_and_clear/2lvl] for
  2-level x86 and [pte_get_and_clear()][pte_get_and_clear/3lvl] for 3-level
  ([PAE][PAE]) x86.

* `pgd_alloc(mm)` - Given a process's `mm`, a [struct mm_struct][mm_struct],
  allocate a physical page for a PGD table, zero
  [USER_PTRS_PER_PGD][USER_PTRS_PER_PGD] entries and copy over entries from
  [swapper_pg_dir][swapper_pg_dir] to populate the rest of the entries (in x86
  at least) - [pgd_alloc()][pgd_alloc]. Typically this calls
  [get_pgd_fast()][get_pgd_fast] in turn, resorting to
  [get_pgd_slow()][get_pgd_slow] if necessary - [pgd_alloc()][pgd_alloc]. Note
  this allocates a PGD table _page_.

* `pmd_alloc(mm, pgd, address)` - Given a process's `mm`, a
  [struct mm_struct][mm_struct], `pgd` a pointer to an entry in a PGD table
  which is to contain the PMD, and `address`, a virtual address, simply returns
  [pmd_offset()][pmd_offset] on i386 - i.e. since all PMDs are either folded
  into the PGD (2-level), or are allocated along with PGD, the function simply
  returns a the virtual address of the associated `pmd_t` -
  [pmd_alloc()][pmd_alloc]. Note this allocates a PMD table _page_.

* `pte_alloc(mm, pmd, address)` - Given a process's `mm`, a
  [struct mm_struct][mm_struct], `pmd` a pointer to an entry in a PMD table
  which is to contain the PTE, and `address`, a virtual address, allocates a new
  PTE table if the PMD entry is empty via
  [pte_alloc_one_fast()][pte_alloc_one_fast] if possible (which uses
  [pte_quicklist][pte_quicklist] in a way very similar to
  [get_pgd_fast()][get_pgd_fast], otherwise it resorts to
  [pte_alloc_one()][pte_alloc_one] to allocate the slow way -
  [pte_alloc()][pte_alloc]. Note this allocates a PTE table _page_.

* `pgd_free(pgd)` - Given `pgd` a pointer to a PGD table, frees the associated
  page. In the PAE case, PMD entries are also freed. In i386 is just an alias
  for [free_pgd_slow()][free_pgd_slow] - [pgd_free()][pgd_free].

* `pmd_free(pmd)` - In (2.4.22 :) i386 this is a no-op. In the 2-level page
  table case PMDs don't exist, in the 3-level case (PAE), PMDs are cleared via
  `pgd_free()` - [pmd_free()][pmd_free].

* `pte_free(pte)` - Given `pte`, a pointer to a PTE table, frees the associated
  page. In i386 this is just an alias for [pte_free_fast()][pte_free_fast],
  which simply adds the PTE entry back to the cache's free list -
  [pte_free()][pte_free].

## Introduction

* Linux has a unique means of abstracting the architecture-specific details of
  physical pages. It maintains a concept of a three-level page table (note: this
  is relevant to 2.4.22 of course, in later kernels it is [4 levels][4level].)
  The 3 levels are:

1. Page Global Directory (PGD)

2. Page Middle Directory (PMD)

3. Page Table Entry (PTE)

* This three-level page table structure is implemented _even if_ the underlying
  architecture does not support it.

* Though this is conceptually easy to understand, it means the distinction
  between different types of pages is really blurry, and page types are
  identified by their flags or what lists they reside in rather than the objects
  they belong to.

* Architectures that manage their [Memory Management Unit (MMU)][mmu]
  differently are expected to emulate the three-level page tables. E.g. on x86
  without [PAE][PAE], only two page table levels are available, so the PMD is
  defined to be of size 1 (2^0) and 'folds back' directly onto the PGD, and this
  is optimised out at compile time.

* For architectures that do not manage their cache or
  [Translation lookaside buffer (TLB)][tlb] automatically,
  architecture-dependent hooks have to be left in the code for when the TLB and
  CPU caches need to be altered and flushed, even if they are null operations on
  some architectures like the x86 (discussed further in section 3.8.)

* Virtual memory is divided into separate directories so we can have a 'sparse'
  set of data structures for each process. Each PGD takes up a page of memory
  (4KiB on i386), rather than the 1MiB it would take to map the whole 4GiB
  address space if it were only a single list.

## 3.1 Describing the Page Directory

* Each process has a pointer [mm_struct][mm_struct]`->pgd` to its own PGD, which
  is a physical page frame containing an array of type [pgd_t][pgd_t], an
  architecture specific type defined in `asm/page.h`:

```c
struct mm_struct {
    struct vm_area_struct * mmap;           /* list of VMAs */
    rb_root_t mm_rb;
    struct vm_area_struct * mmap_cache;     /* last find_vma result */
    pgd_t * pgd;
    atomic_t mm_users;                      /* How many users with user space? */
    atomic_t mm_count;                      /* How many references to "struct mm_struct" (users count as 1) */
    int map_count;                          /* number of VMAs */
    struct rw_semaphore mmap_sem;
    spinlock_t page_table_lock;             /* Protects task page tables and mm->rss */

    struct list_head mmlist;                /* List of all active mm's.  These are globally strung
                                             * together off init_mm.mmlist, and are protected
                                             * by mmlist_lock
                                             */

    unsigned long start_code, end_code, start_data, end_data;
    unsigned long start_brk, brk, start_stack;
    unsigned long arg_start, arg_end, env_start, env_end;
    unsigned long rss, total_vm, locked_vm;
    unsigned long def_flags;
    unsigned long cpu_vm_mask;
    unsigned long swap_address;

    unsigned dumpable:1;

    /* Architecture-specific MM context */
    mm_context_t context;
};
```

* Page tables are loaded differently depending on architecture. On x86, process
  page table is loaded by copying `mm_struct->pgd` into the `cr3` register,
  which has the side effect of flushing the [TLB][tlb].

* In fact, on x86, [__flush_tlb()][__flush_tlb] is implemented by copying `cr3`
  into a temporary register, then copying that result back in again.

* Each _active_ entry in the PGD table points to a page frame (i.e. physical
  page of memory) containing an array of PMD entries of type [pmd_t][pmd_t],
  which in term point to page frames containing PTEs of type [pte_t][pte_t],
  which finally point to page frames containing the actual user data.

* The following code defines how we can determine a physical address manually
  (of course in the majority of cases access is performed transparently by the
  MMU):

```c
/* Shared between 2-level and 3-level i386: */

#define pgd_offset(mm, address) ((mm)->pgd+pgd_index(address))
#define pgd_index(address) ((address >> PGDIR_SHIFT) & (PTRS_PER_PGD-1))
#define pgd_page(pgd) \
	((unsigned long) __va(pgd_val(pgd) & PAGE_MASK))
#define __va(x) ((void *)((unsigned long)(x)+PAGE_OFFSET))
/*
 * Note that pgd_t is a struct containing just the pgd pointer, used to take
 * advantage of C type checking.
 */
#define pgd_val(x) ((x).pgd)

#define PAGE_SHIFT   12 /* 4 KiB. */
#define PAGE_SIZE    (1UL << PAGE_SHIFT)
#define PAGE_MASK    (~(PAGE_SIZE-1))

#define __pte_offset(address) \
		((address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
#define pte_offset(dir, address) ((pte_t *) pmd_page(*(dir)) + \
			__pte_offset(address))

#define pmd_page(pmd) \
	((unsigned long) __va(pmd_val(pmd) & PAGE_MASK))

/* In 2-level (non-PAE) configuration: */

#define PGDIR_SHIFT  22
#define PTRS_PER_PGD 1024

#define PMD_SHIFT    22
#define PTRS_PER_PMD 1

#define PTRS_PER_PTE 1024

static inline pmd_t * pmd_offset(pgd_t * dir, unsigned long address)
{
	return (pmd_t *) dir;
}

/* In 3-level (PAE) configuration: */

#define PGDIR_SHIFT  30
#define PTRS_PER_PGD 4

#define PMD_SHIFT    21
#define PTRS_PER_PMD 512

#define PTRS_PER_PTE 512

#define pmd_offset(dir, address) ((pmd_t *) pgd_page(*(dir)) + \
		__pmd_offset(address))
#define __pmd_offset(address) \
		(((address) >> PMD_SHIFT) & (PTRS_PER_PMD-1))
```

* [pgd_offset()][pgd_offset] Determines the _physical address_ of the PGD entry
  for a given virtual address given that address and the process's
  [mm_struct][mm_struct], via [pgd_index()][pgd_index]. We shift the address
  right by [PGDIR_SHIFT][PGDIR_SHIFT/2lvl] bits, before limiting to the number
  of possible pointers, [PTRS_PER_PGD][PTRS_PER_PGD/2lvl]. This number will be a
  power of 2, so the number subtracted by 1 is a mask of all possible indexes
  into the PGD, e.g. `1000` allows all permutations of `[0000,0111]`. Finally we
  add the known physical address of the pgd from `mm->pgd`.

### 2-level i386

* The structure is a PGD table with 1024 entries -> PTE table with 1024
  entries. The PMD is just an alias for the PGD.

* In a 2-level i386 configuration, [PGDIR_SHIFT][PGDIR_SHIFT/2lvl] is 22 and
  [PTRS_PER_PGD][PTRS_PER_PGD/2lvl] is 1024 so we use the upper 10 bits to
  offset into 1024 entries. [pmd_offset][pmd_offset/2lvl] simply returns a
  (physical) pointer to the PGD entry, so when we translate to PMD, we are just
  looking at the same PGD entry again. The compiler will remove this unnecessary
  intermediate step, but we retain the same abstraction in generic code. Very
  clever!

* Note that the input `dir` value is a _virtual_ address of type `pgd_t *`,
  whose value is a _physical_ address for a PGD _entry_, so the returned `pmd_t
  *` is exactly the same - a virtual address which contains a physical address
  value.

* Now we have the physical address of our 'PMD' entry (i.e. the same PGD entry),
  we can use this to determine our PTE and finally get hold of the address of
  the physical page we want, exactly the same as we would with a 3-level system.

### 3-level i386 (PAE)

* The structure is a PGD table with only 4 entries -> PMD table with 512 64-bit
  entries (to store more address bits) -> PTE table with 512 entries.

* In a 3-level i386 PAE configuration, [PGDIR_SHIFT][PGDIR_SHIFT/3lvl] is 30 and
  [PTRS_PER_PGD][PTRS_PER_PGD/3lvl] is 4 and we have an actually real PMD :) to
  get it, we use [pmd_offset()][pmd_offset/3lvl], which starts by calling
  [pgd_page()][pgd_page] to get the _virtual_ address of the physical PMD
  contained inside the PGD entry. It starts by calling [pgd_val()][pgd_val]:

* `pgd_val()` gets the physical address of the specified PGD _entry_ -
  [pgd_t][pgd_t] is actually a `struct` containing a physical page address to
  make use of C type checking, so it does this by simply accessing the member.

* Now we come back to `pgd_page()` which uses [PAGE_MASK][PAGE_MASK] (this masks
  out the lower 12 bits, i.e. 4KiB's worth) to get the physical page of the PMD,
  ignoring any flags contained in the PGD entry, then uses [__va][__va] to
  convert this physical _kernel_ address into a virtual _kernel_ address,
  i.e. simply offsetting by [__PAGE_OFFSET][__PAGE_OFFSET] (kernel addresses are
  mapped directly like this.)

* Finally now we have a virtual address for the start of the PMD entry table,
  `pmd_offset()` needs to get the offset of the specific `pmd_t` we're after in
  order to return a `pmd_t *` virtual address to it. It does this via
  [__pmd_offset][__pmd_offset] which does the same job as
  [pgd_index()][pgd_index], only using [PMD_SHIFT][PMD_SHIFT/3lvl] (21) and
  [PTRS_PER_PMD][PTRS_PER_PMD/3lvl] (512.) It is a 21-bit shift because we've
  used 2 bits for the PMD entry, leaving us with 9 bits to address 512 entries.

### (LS) Looking up the PTE

* Similar to [pmd_offset()][pmd_offset/3lvl], [pte_offset()][pte_offset] grabs
  the contents of the PMD entry via [pmd_page()][pmd_page] (in a 2-level system
  this will just be the same as [pgd_page()][pgd_page]), using
  [pmd_val()][pmd_val] to grab the `pmd` field from the [pmd_t][pmd_t] and
  finally return a virtual `pte_t *` value. Finally, it returns the _physical_
  address of the PTE as a [pte_t *][pte_t].

* In a 3-level ([PAE][PAE]) system, the [pte_t][pte_t] is a 64-bit value, meaning
  [pte_val()][pte_val/3lvl] returns a 64-bit physical address.

### (LS) Some additional functions/values

* [PAGE_SIZE][PAGE_SIZE] - Size of each page of memory.

* [PMD_SIZE][PMD_SIZE] - The size of values that are mapped by a PMD, which is
  derived from [PMD_SHIFT][PMD_SHIFT/3lvl] which determines the number of bits
  mapped by a PMD (`PMD_SIZE = 1 << PMD_SHIFT`.)

* [PMD_MASK][PMD_MASK] - The mask for the *upper bits* of a PMD address,
  i.e. PGD+PMD.

* [PGDIR_SIZE][PGDIR_SIZE], [PGDIR_MASK][PGDIR_MASK] - Similar to PMD
equivalents for the PGD.

* If a page needs to be aligned on a page boundary, [PAGE_ALIGN()][PAGE_ALIGN]
  adds `PAGE_SIZE - 1` to the address then simply bitwise ands
  [PAGE_MASK][PAGE_MASK] - since `PAGE_MASK` is a mask for the *high* bits of an
  address without the lower page offset bits, this means that addresses that are
  already page aligned will remain the same (since it's `PAGE_SIZE - 1`), and
  any other addresses will be moved to the next page.

* Somewhat discussed above, but [PTRS_PER_PGD][PTRS_PER_PGD/3lvl],
  [PTRS_PER_PMD][PTRS_PER_PMD/3lvl] and [PTRS_PER_PTE][PTRS_PER_PTE/3lvl] each
  describe the number of pointers provided in each table.

## 3.2 Describing a Page Table Entry

* Each entry in the page tables are described by [pgd_t][pgd_t], [pmd_t][pmd_t]
  and [pte_t][pte_t]. Though the values the entries contain are unsigned
  integers, they are defined as structs. This is firstly for type safety, to
  ensure these are not used incorrectly, and secondly to handle e.g. PAE which
  uses an additional 4 bits for addressing.

* As seen above, type casting from these structs to unsigned integers is done
  via [pgd_val()][pgd_val], [pmd_val()][pmd_val] and [pte_val()][pte_val/3lvl].

* Reversing the process, i.e. converting from an unsigned integer to the
  appropriate struct is done via [__pte()][__pte], [__pmd()][__pmd] and
  [__pgd()][__pgd].

* Each PTE entry has protection bits, for which [pgprot_t][pgprot_t] is used.

* Converting to and from `pgprot_t` is similar to the page table entries -
  [pgprot_val()][pgprot_val] and [__pgprot()][__pgprot].

* Where the protection bits are actually stored is architecture-dependent,
  however on 32-bit x86 non-PAE, [pte_t][pte_t] is just a 32-bit integer in a
  struct. Since each `pte_t` points to an address of a page frame, which is
  guaranteed to be page-aligned, we are left with [PAGE_SHIFT][PAGE_SHIFT] bits
  free for status bits.

* Some of the potential status values are:

1. [_PAGE_PRESENT][_PAGE_PRESENT] - The page is resident in memory and not
   swapped out.
2. [_PAGE_RW][_PAGE_RW] - Page can be written to.
3. [_PAGE_USER][_PAGE_USER] - Page is accessible from user space.
4. [_PAGE_ACCESSED][_PAGE_ACCESSED] - Page has been accessed.
5. [_PAGE_DIRTY][_PAGE_DIRTY] - Page has been written to.
6. [_PAGE_PROTNONE][_PAGE_PROTNONE] - The page is resident, but not accessible.

* The bit referenced by `_PAGE_PROTNONE` is, in pentium III processors and
  higher, referred to as the 'Page Attribute Table' (PAT) bit and used to
  indicate the size of the page that the PTE is referencing. Equivalently, this
  bit is referred to as the 'Page Size Extension' (PSE) bit.

* Linux doesn't use either PAT or PSE in userspace, so this bit is free for
  other uses. It's used to fulfil cases like an [mprotect()][mprotect]'d region
  of memory set to `PROT_NONE` where the memory needs to resident, but
  inaccessible to userland. This is achieved by clearing `_PAGE_PRESENT` and
  setting `_PAGE_PROTNONE`.

* Since `_PAGE_PRESENT` is clear, the hardware will invoke a fault if userland
  tries to access it, but the [pte_present()][pte_present] macro will detect
  that `PAGE_PROTNONE` is defined so the kernel will know to keep this page
  resident.

## 3.3 Using Page Table Entries

* (See the cheat sheet for details on useful functions.)

* There is a lot of page table walk code in the VM, and it's important to be
able to recognise it. For example, [follow_page()][follow_page] from
`mm/memory.c`:

```c
/*
 * Do a quick page-table lookup for a single page.
 */
static struct page * follow_page(struct mm_struct *mm, unsigned long address, int write)
{
	pgd_t *pgd;
	pmd_t *pmd;
	pte_t *ptep, pte;

	pgd = pgd_offset(mm, address);
	if (pgd_none(*pgd) || pgd_bad(*pgd))
		goto out;

	pmd = pmd_offset(pgd, address);
	if (pmd_none(*pmd) || pmd_bad(*pmd))
		goto out;

	ptep = pte_offset(pmd, address);
	if (!ptep)
		goto out;

	pte = *ptep;
	if (pte_present(pte)) {
		if (!write ||
		    (pte_write(pte) && pte_dirty(pte)))
			return pte_page(pte);
	}

out:
	return 0;
}
```

* We first grab a pointer to the PGD entry, check to make sure it isn't empty
  and isn't bad, use this to grab a pointer to the PMD entry, again checking for
  validity before finally grabbing a pointer to the PTE entry which we
  dereference and place a copy in local variable `pte`.

* After this we are done with our walking code and use [pte_page()][pte_page] to
  return the PTE's associated [struct page][page] if the present bit is set and
  if the `write` argument is truthy, checking whether the page is writeable and
  dirty before returning it.

## 3.4 Translating and Setting Page Table Entries

* (See the cheat sheet for details on useful functions.)

## 3.5 Allocating and Freeing Page Tables

* (See the cheat sheet for details on useful functions.)

* The allocation and freeing of page tables is a relatively expensive operation,
  both in terms of time and the fact that interrupts are disabled during page
  allocation.

* In addition, the allocation and deletion of page tables at any of the 3 levels
is very frequent, so it's important that the operation is as quick as possible.

* The pages used for page tables are therefore cached in a number of different
  lists known as _quicklists_.

* The implementation of these differ between architectures, e.g. not all cache
  PGDs because allocating and freeing them only happens during process
  creation/exit, and since these are expensive operations, the allocation of
  another page is negligible.

* Allocation of pages is achieved via [pgd_alloc()][pgd_alloc],
  [pmd_alloc()][pmd_alloc], and [pte_alloc()][pte_alloc].

* Freeing of pages is achieved via [pgd_free()][pgd_free],
  [pmd_free()][pmd_free], and [pte_free()][pte_free].

* Generally speaking, caching is implemented for each of the page tables using
  [pgd_quicklist][pgd_quicklist], [pmd_quicklist][pmd_quicklist], and
  [pte_quicklist][pte_quicklist].

* These lists are defined in an architecture-dependent way but one means this is
  achieved is via a LIFO list (i.e. a stack.) When a page is adding back to the
  cache (i.e. is freed), it becomes the head of the list and the cache size
  count is increment. A pointer to next page in the list is stored at the start
  of the page, e.g.:

```c
static inline void pte_free_fast(pte_t *pte)
{
        *(unsigned long *)pte = (unsigned long) pte_quicklist;
        pte_quicklist = (unsigned long *) pte;
        pgtable_cache_size++;
}
```

* The quick allocation function from `pgd_quicklist` is not necessarily the same
  for each architecture, however commonly, [get_pgd_fast()][get_pgd_fast] is
  used, which pops an entry off the free list if possible, otherwise allocates
  via [get_pgd_slow()][get_pgd_slow]:

```c
static inline pgd_t *get_pgd_fast(void)
{
        unsigned long *ret;

        if ((ret = pgd_quicklist) != NULL) {
                pgd_quicklist = (unsigned long *)(*ret);
                ret[0] = 0;
                pgtable_cache_size--;
        } else
                ret = (unsigned long *)get_pgd_slow();
        return (pgd_t *)ret;
}
```

* Here you can see the dereference of the next element in the list, followed by
clearing this from the page and decrementing the count.

* `pmd_alloc_one` and `pmd_alloc_one_fast` are not implemented in i386 as PMD
  handling is wrapped inside of the PGD.

* On the slow path, [get_pgd_slow()][get_pgd_slow] and
  [pte_alloc_one()][pte_alloc_one] are used to allocate these table pages when
  the cache doesn't have any free entries.

* Obviously over time these caches grow rather large. As the pages grow and
  shrink a counter is incremented/decremented, and there are high and low
  watermarks for this counter.

* [check_pgt_cache()][check_pgt_cache] is called from two different places to
  check these watermarks - the system idle task and after
  [clear_page_tables()][clear_page_tables] is run. If the high watermark is
  reached, pages are freed until the size returns to the low watermark.

[4level]:https://lwn.net/Articles/117749/
[mmu]:https://en.wikipedia.org/wiki/Memory_management_unit
[tlb]:https://en.wikipedia.org/wiki/Translation_lookaside_buffer
[mm_struct]:http://fxr.watson.org/fxr/source/include/linux/sched.h?v=linux-2.4.22#L206
[pgd_t]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22;#L42
[__flush_tlb]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L38
[pmd_t]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L41
[pte_t]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L40
[pgd_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L331
[pgd_index]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L327
[PGDIR_SHIFT/2lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L8
[PTRS_PER_PGD/2lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L9
[PGDIR_SHIFT/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L14
[PTRS_PER_PGD/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L15
[pmd_offset/2lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L55
[pmd_offset/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L72
[pgd_page]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L52
[PAGE_MASK]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L7
[pgd_val]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L55
[__va]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L133
[PAE]:https://en.wikipedia.org/wiki/Physical_Address_Extension
[__pmd_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L336
[PMD_SHIFT/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L21
[PTRS_PER_PMD/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L22
[pte_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L342
[__pte_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L340
[pmd_page]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L323
[pte_val/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L43
[pmd_val]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L54
[__PAGE_OFFSET]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L81
[pte_page]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L92
[page]:http://fxr.watson.org/fxr/source/include/linux/mm.h?v=linux-2.4.22#L154
[PAGE_SIZE]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L6
[PMD_SIZE]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L136
[PMD_MASK]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L137
[PGDIR_SIZE]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L138
[PGDIR_MASK]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L139
[PAGE_ALIGN]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L66
[PAGE_MASK]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L7
[PTRS_PER_PTE/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L27
[pgprot_t]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L52
[__pte]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L58
[__pmd]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L59
[__pgd]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L60
[__pgprot]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L61
[pgprot_val]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L56
[PAGE_SHIFT]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L5
[_PAGE_PRESENT]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L187
[_PAGE_RW]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L188
[_PAGE_USER]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L189
[_PAGE_ACCESSED]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L192
[_PAGE_DIRTY]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L193
[_PAGE_PROTNONE]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L197
[mprotect]:http://man7.org/linux/man-pages/man2/mprotect.2.html
[pte_present]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L267
[pmd_none]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L270
[pmd_present]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L271
[pmd_clear]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L272
[pmd_bad]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L273
[follow_page]:http://fxr.watson.org/fxr/source/mm/memory.c?v=linux-2.4.22#L405

[pte_dirty]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L284
[pte_mkdirty]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L295
[pte_mkclean]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L290

[pte_young]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L285
[pte_mkyoung]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L296
[pte_mkold]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L291

[pte_read]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L282
[pte_mkread]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L293
[pte_rdprotect]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L288

[pte_write]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L286
[pte_mkwrite]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L297
[pte_wrprotect]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L292

[pte_exec]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L283
[pte_mkexec]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L294
[pte_exprotect]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L289

[nx-bit]:https://en.wikipedia.org/wiki/NX_bit

[mk_pte]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L309
[__mk_pte/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22;im=3#L95
[mk_pte_phys]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L312

[set_pte/2lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L42
[set_pte/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L46
[pte_clear]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L268
[pte_get_and_clear/2lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L59
[pte_get_and_clear/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L75

[pgd_alloc]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L144
[pmd_alloc]:http://fxr.watson.org/fxr/source/include/linux/mm.h?v=linux-2.4.22#L511
[pte_alloc]:http://fxr.watson.org/fxr/source/mm/memory.c?v=linux-2.4.22#L1431
[pgd_free]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L143
[pmd_free]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L156
[pte_free]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L142
[free_pgd_slow]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L94
[pte_free_fast]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L130

[get_pgd_fast]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L74
[get_pgd_slow]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L32

[pgd_quicklist]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L9
[pmd_quicklist]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L10
[pte_quicklist]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L11
[pte_alloc_one_fast]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22#L117
[pte_alloc_one]:http://fxr.watson.org/fxr/source/include/asm-i386/pgalloc.h?v=linux-2.4.22;im=excerpts#L107

[check_pgt_cache]:http://fxr.watson.org/fxr/source/mm/memory.c?v=linux-2.4.22#L136
[clear_page_tables]:http://fxr.watson.org/fxr/source/mm/memory.c?v=linux-2.4.22#L146
