# Chapter 3: Page Table Management

## (LS) Overview

In linux 2.4.22 a 32-bit virtual address actually consists of 4 offsets,
indexing into the PGD, PMD, PTE and the address's actual physical page of
memory (the PGD, PMD and PTE are defined below.)

Each process has a known PGD address. Subsequently (noting these are all
_physical_ addresses):

```
pmd_table = pgd_table[pgd_offset] & PAGE_MASK
pte_table = pmd_table[pmd_offset] & PAGE_MASK
phys_page = pte_table[pte_offset] & PAGE_MASK
phys_addr = phys_page[page_offset]
```

__NOTE:__ This is pseudo-code.

__NOTE:__ `PAGE_MASK = ~(1<<12 - 1)` excludes the least significant bits since
pages are page-aligned.

This indirection exists in order that each process need only take up 1 physical
page of memory for its overall page directory - i.e. entries can be empty and
are filled as needed (the layout is 'sparse'.)

A virtual address therefore looks like:

```
<-                    BITS_PER_LONG                      ->
<- PGD bits -><- PMD bits -><- PTE bits -><- Offset bits ->
[ PGD Offset ][ PMD Offset ][ PTE Offset ][  Page Offset  ]
                                          <----- PAGE_SHIFT
                            <-------------------- PMD_SHIFT
              <-------------------------------- PGDIR_SHIFT
```

As discussed below, it's possible for offsets to be of 0 bits length, meaning
the directory in question is folded back onto the nearest higher directory.

In 32-bit non-[PAE][PAE] we have:

```
<-  10 bits -><-  0 bits  -><- 10 bits  -><-   12 bits   ->
[ PGD Offset ][ PMD Offset ][ PTE Offset ][  Page Offset  ]
```

In 32-bit [PAE][PAE] we have:

```
<-  2 bits  -><-  9 bits  -><-  9 bits  -><-   12 bits   ->
[ PGD Offset ][ PMD Offset ][ PTE Offset ][  Page Offset  ]
```

Importantly here all physical pages are stored as 64-bit values. A 32-bit intel
PAE configuration could physically address 36 bits (64GB) so only 4 bits of the
higher order byte are used for addressing (some of the other bits are used for
flags, see the [PAE wikipedia article][PAE] for more details.)

## (LS) Page Table Function Cheat Sheet

* `pgd_offset(mm, address)` - Given a process's [struct mm_struct][mm_struct]
  (e.g. for the current process this would be `current->mm`) and a _virtual_
  `address`, returns the _virtual address_ of the corresponding PGD
  _entry_ (i.e. a `pgd_t *`.)

* `pmd_offset(dir, address)` - Given `dir`, a _virtual_ address of type `pgd_t
  *`, i.e. a pointer to the _entry_ which contains the PMD table's _physical_
  address, and a _virtual_ `address`, returns the virtual address of the
  corresponding PMD entry of type `pmd_t *`.

* `pte_offset(dir, address)` - Given `dir`, a _virtual_ address of type `pmd_t
  *`, i.e. a pointer to the _entry_ which contains the PTE table's _physical_
  address, and a _virtual_ `address`, returns the virtual address of the
  corresponding PTE entry of type `pte_t *`.

* `__[pgd,pmd,pte]_offset(address)` - Given a _virtual_ `address`, returns the
  index of the `[pgd,pmd,pte]_t` entry for this address within the PGD, PMD,
  PTE.

* `pgd_index(address)` - Given the _virtual_ `address`, [pgd_index()][pgd_index]
  returns the index of the [pgd_t][pgd_t] entry for this address within the
  PGD. Equivalent to `__pgd_offset(address)`.

* `[pgd,pmd]_page(entry)` - Given `entry`, a page table entry containing a
  _physical_ address of type `[pgd,pmd]_t`, returns the _virtual_ address of the
  page referenced by this entry. So [pgd_page()][pgd_page] returns `pmd_t
  *pmd_table` and [pmd_page()][pmd_page] returns `pte_t *pte_table`.

* `pte_page(entry)` - Given `entry`, a page table entry containing a _physical_
  address of type `pte_t`, [pte_page()][pte_page] returns the [struct
  page][page] entry (also typedef'd to `mem_map_t`) for the referenced page.

* `[pgd,pmd,pte]_val(entry)` - Given `entry`, a page table entry containing a
  _physical_ address of type `[pgd,pmd,pte]_t`, returns the absolute value of
  the physical address (i.e. `unsigned long` or `unsigned long long`.) Note that
  when using [PAE][PAE] this will be a 64-bit value.

* `[pgd,pmd,pte]_none(entry)` - Given `entry`, a page table entry containing a
  _physical_ address of type `[pgd,pmd,pte]_t`, returns true if it does not
  exist, e.g. [pmd_none()][pmd_none].

* `[pgd,pmd,pte]_present(entry)` - Given `entry`, a page table entry containing
  a _physical_ address of type `[pgd,pmd,pte]_t`, returns true if it has the
  present bit set e.g. [pmd_present()][pmd_present].

* `[pgd,pmd,pte]_clear(entry)` - Given `entry`, a page table entry containing a
  _physical_ address of type `[pgd,pmd,pte]_t`, clears the entry e.g.
  [pmd_clear()][pmd_clear].

* `[pgd,pmd]_bad(entry)` - Given `entry`, a page table entry containing a
  _physical_ address of type `[pgd,pmd]_t`, returns true if the entry is not in
  a state where the contents can be safely modified, e.g. [pmd_bad()][pmd_bad].

## Introduction

* Linux has a unique means of abstracting the architecture-specific details of
  physical pages. It maintains a concept of a three-level page table (note: this
  is relevant to 2.4.22 of course, in later kernels it is [4 levels][4level].)
  The 3 levels are:

1. Page Global Directory (PGD)

2. Page Middle Directory (PMD)

3. Page Table Entry (PTE)

* This three-level page table structure is implemented _even if_ the underlying
  architecture does not support it.

* Though this is conceptually easy to understand, it means the distinction
  between different types of pages is really blurry, and page types are
  identified by their flags or what lists they reside in rather than the objects
  they belong to.

* Architectures that manage their [Memory Management Unit (MMU)][mmu]
  differently are expected to emulate the three-level page tables. E.g. on x86
  without [PAE][PAE], only two page table levels are available, so the PMD is
  defined to be of size 1 (2^0) and 'folds back' directly onto the PGD, and this
  is optimised out at compile time.

* For architectures that do not manage their cache or
  [Translation lookaside buffer (TLB)][tlb] automatically,
  architecture-dependent hooks have to be left in the code for when the TLB and
  CPU caches need to be altered and flushed, even if they are null operations on
  some architectures like the x86 (discussed further in section 3.8.)

* Virtual memory is divided into separate directories so we can have a 'sparse'
  set of data structures for each process. Each PGD takes up a page of memory
  (4KiB on i386), rather than the 1MiB it would take to map the whole 4GiB
  address space if it were only a single list.

## 3.1 Describing the Page Directory

* Each process has a pointer [mm_struct][mm_struct]`->pgd` to its own PGD, which
  is a physical page frame containing an array of type [pgd_t][pgd_t], an
  architecture specific type defined in `asm/page.h`:

```c
struct mm_struct {
    struct vm_area_struct * mmap;           /* list of VMAs */
    rb_root_t mm_rb;
    struct vm_area_struct * mmap_cache;     /* last find_vma result */
    pgd_t * pgd;
    atomic_t mm_users;                      /* How many users with user space? */
    atomic_t mm_count;                      /* How many references to "struct mm_struct" (users count as 1) */
    int map_count;                          /* number of VMAs */
    struct rw_semaphore mmap_sem;
    spinlock_t page_table_lock;             /* Protects task page tables and mm->rss */

    struct list_head mmlist;                /* List of all active mm's.  These are globally strung
                                             * together off init_mm.mmlist, and are protected
                                             * by mmlist_lock
                                             */

    unsigned long start_code, end_code, start_data, end_data;
    unsigned long start_brk, brk, start_stack;
    unsigned long arg_start, arg_end, env_start, env_end;
    unsigned long rss, total_vm, locked_vm;
    unsigned long def_flags;
    unsigned long cpu_vm_mask;
    unsigned long swap_address;

    unsigned dumpable:1;

    /* Architecture-specific MM context */
    mm_context_t context;
};
```

* Page tables are loaded differently depending on architecture. On x86, process
  page table is loaded by copying `mm_struct->pgd` into the `cr3` register,
  which has the side effect of flushing the [TLB][tlb].

* In fact, on x86, [__flush_tlb()][__flush_tlb] is implemented by copying `cr3`
  into a temporary register, then copying that result back in again.

* Each _active_ entry in the PGD table points to a page frame (i.e. physical
  page of memory) containing an array of PMD entries of type [pmd_t][pmd_t],
  which in term point to page frames containing PTEs of type [pte_t][pte_t],
  which finally point to page frames containing the actual user data.

* The following code defines how we can determine a physical address manually
  (of course in the majority of cases access is performed transparently by the
  MMU):

```c
/* Shared between 2-level and 3-level i386: */

#define pgd_offset(mm, address) ((mm)->pgd+pgd_index(address))
#define pgd_index(address) ((address >> PGDIR_SHIFT) & (PTRS_PER_PGD-1))
#define pgd_page(pgd) \
	((unsigned long) __va(pgd_val(pgd) & PAGE_MASK))
#define __va(x) ((void *)((unsigned long)(x)+PAGE_OFFSET))
/*
 * Note that pgd_t is a struct containing just the pgd pointer, used to take
 * advantage of C type checking.
 */
#define pgd_val(x) ((x).pgd)

#define PAGE_SHIFT   12 /* 4 KiB. */
#define PAGE_SIZE    (1UL << PAGE_SHIFT)
#define PAGE_MASK    (~(PAGE_SIZE-1))

#define __pte_offset(address) \
		((address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
#define pte_offset(dir, address) ((pte_t *) pmd_page(*(dir)) + \
			__pte_offset(address))

#define pmd_page(pmd) \
	((unsigned long) __va(pmd_val(pmd) & PAGE_MASK))

/* In 2-level (non-PAE) configuration: */

#define PGDIR_SHIFT  22
#define PTRS_PER_PGD 1024

#define PMD_SHIFT    22
#define PTRS_PER_PMD 1

#define PTRS_PER_PTE 1024

static inline pmd_t * pmd_offset(pgd_t * dir, unsigned long address)
{
	return (pmd_t *) dir;
}

/* In 3-level (PAE) configuration: */

#define PGDIR_SHIFT  30
#define PTRS_PER_PGD 4

#define PMD_SHIFT    21
#define PTRS_PER_PMD 512

#define PTRS_PER_PTE 512

#define pmd_offset(dir, address) ((pmd_t *) pgd_page(*(dir)) + \
		__pmd_offset(address))
#define __pmd_offset(address) \
		(((address) >> PMD_SHIFT) & (PTRS_PER_PMD-1))
```

* [pgd_offset()][pgd_offset] Determines the _physical address_ of the PGD entry
  for a given virtual address given that address and the process's
  [mm_struct][mm_struct], via [pgd_index()][pgd_index]. We shift the address
  right by [PGDIR_SHIFT][PGDIR_SHIFT/2lvl] bits, before limiting to the number
  of possible pointers, [PTRS_PER_PGD][PTRS_PER_PGD/2lvl]. This number will be a
  power of 2, so the number subtracted by 1 is a mask of all possible indexes
  into the PGD, e.g. `1000` allows all permutations of `[0000,0111]`. Finally we
  add the known physical address of the pgd from `mm->pgd`.

### 2-level i386

* The structure is a PGD table with 1024 entries -> PTE table with 1024
  entries. The PMD is just an alias for the PGD.

* In a 2-level i386 configuration, [PGDIR_SHIFT][PGDIR_SHIFT/2lvl] is 22 and
  [PTRS_PER_PGD][PTRS_PER_PGD/2lvl] is 1024 so we use the upper 10 bits to
  offset into 1024 entries. [pmd_offset][pmd_offset/2lvl] simply returns a
  (physical) pointer to the PGD entry, so when we translate to PMD, we are just
  looking at the same PGD entry again. The compiler will remove this unnecessary
  intermediate step, but we retain the same abstraction in generic code. Very
  clever!

* Note that the input `dir` value is a _virtual_ address of type `pgd_t *`,
  whose value is a _physical_ address for a PGD _entry_, so the returned `pmd_t
  *` is exactly the same - a virtual address which contains a physical address
  value.

* Now we have the physical address of our 'PMD' entry (i.e. the same PGD entry),
  we can use this to determine our PTE and finally get hold of the address of
  the physical page we want, exactly the same as we would with a 3-level system.

### 3-level i386 (PAE)

* The structure is a PGD table with only 4 entries -> PMD table with 512 64-bit
  entries (to store more address bits) -> PTE table with 512 entries.

* In a 3-level i386 PAE configuration, [PGDIR_SHIFT][PGDIR_SHIFT/3lvl] is 30 and
  [PTRS_PER_PGD][PTRS_PER_PGD/3lvl] is 4 and we have an actually real PMD :) to
  get it, we use [pmd_offset()][pmd_offset/3lvl], which starts by calling
  [pgd_page()][pgd_page] to get the _virtual_ address of the physical PMD
  contained inside the PGD entry. It starts by calling [pgd_val()][pgd_val]:

* `pgd_val()` gets the physical address of the specified PGD _entry_ -
  [pgd_t][pgd_t] is actually a `struct` containing a physical page address to
  make use of C type checking, so it does this by simply accessing the member.

* Now we come back to `pgd_page()` which uses [PAGE_MASK][PAGE_MASK] (this masks
  out the lower 12 bits, i.e. 4KiB's worth) to get the physical page of the PMD,
  ignoring any flags contained in the PGD entry, then uses [__va][__va] to
  convert this physical _kernel_ address into a virtual _kernel_ address,
  i.e. simply offsetting by [__PAGE_OFFSET][__PAGE_OFFSET] (kernel addresses are
  mapped directly like this.)

* Finally now we have a virtual address for the start of the PMD entry table,
  `pmd_offset()` needs to get the offset of the specific `pmd_t` we're after in
  order to return a `pmd_t *` virtual address to it. It does this via
  [__pmd_offset][__pmd_offset] which does the same job as
  [pgd_index()][pgd_index], only using [PMD_SHIFT][PMD_SHIFT/3lvl] (21) and
  [PTRS_PER_PMD][PTRS_PER_PMD/3lvl] (512.) It is a 21-bit shift because we've
  used 2 bits for the PMD entry, leaving us with 9 bits to address 512 entries.

### (LS) Looking up the PTE

* Similar to [pmd_offset()][pmd_offset/3lvl], [pte_offset()][pte_offset] grabs
  the contents of the PMD entry via [pmd_page()][pmd_page] (in a 2-level system
  this will just be the same as [pgd_page()][pgd_page]), using
  [pmd_val()][pmd_val] to grab the `pmd` field from the [pmd_t][pmd_t] and
  finally return a virtual `pte_t *` value. Finally, it returns the _physical_
  address of the PTE as a [pte_t *][pte_t].

* In a 3-level ([PAE][PAE]) system, the [pte_t][pte_t] is a 64-bit value, meaning
  [pte_val()][pte_val/3lvl] returns a 64-bit physical address.

### (LS) Some additional functions/values

* [PAGE_SIZE][PAGE_SIZE] - Size of each page of memory.

* [PMD_SIZE][PMD_SIZE] - The size of values that are mapped by a PMD, which is
  derived from [PMD_SHIFT][PMD_SHIFT/3lvl] which determines the number of bits
  mapped by a PMD (`PMD_SIZE = 1 << PMD_SHIFT`.)

* [PMD_MASK][PMD_MASK] - The mask for the *upper bits* of a PMD address,
  i.e. PGD+PMD.

* [PGDIR_SIZE][PGDIR_SIZE], [PGDIR_MASK][PGDIR_MASK] - Similar to PMD
equivalents for the PGD.

* If a page needs to be aligned on a page boundary, [PAGE_ALIGN()][PAGE_ALIGN]
  adds `PAGE_SIZE - 1` to the address then simply bitwise ands
  [PAGE_MASK][PAGE_MASK] - since `PAGE_MASK` is a mask for the *high* bits of an
  address without the lower page offset bits, this means that addresses that are
  already page aligned will remain the same (since it's `PAGE_SIZE - 1`), and
  any other addresses will be moved to the next page.

* Somewhat discussed above, but [PTRS_PER_PGD][PTRS_PER_PGD/3lvl],
  [PTRS_PER_PMD][PTRS_PER_PMD/3lvl] and [PTRS_PER_PTE][PTRS_PER_PTE/3lvl] each
  describe the number of pointers provided in each table.

## 3.2 Describing a Page Table Entry

* Each entry in the page tables are described by [pgd_t][pgd_t], [pmd_t][pmd_t]
  and [pte_t][pte_t]. Though the values the entries contain are unsigned
  integers, they are defined as structs. This is firstly for type safety, to
  ensure these are not used incorrectly, and secondly to handle e.g. PAE which
  uses an additional 4 bits for addressing.

* As seen above, type casting from these structs to unsigned integers is done
  via [pgd_val()][pgd_val], [pmd_val()][pmd_val] and [pte_val()][pte_val/3lvl].

* Reversing the process, i.e. converting from an unsigned integer to the
  appropriate struct is done via [__pte()][__pte], [__pmd()][__pmd] and
  [__pgd()][__pgd].

* Each PTE entry has protection bits, for which [pgprot_t][pgprot_t] is used.

* Converting to and from `pgprot_t` is similar to the page table entries -
  [pgprot_val()][pgprot_val] and [__pgprot()][__pgprot].

* Where the protection bits are actually stored is architecture-dependent,
  however on 32-bit x86 non-PAE, [pte_t][pte_t] is just a 32-bit integer in a
  struct. Since each `pte_t` points to an address of a page frame, which is
  guaranteed to be page-aligned, we are left with [PAGE_SHIFT][PAGE_SHIFT] bits
  free for status bits.

* Some of the potential status values are:

1. [_PAGE_PRESENT][_PAGE_PRESENT] - The page is resident in memory and not
   swapped out.
2. [_PAGE_RW][_PAGE_RW] - Page can be written to.
3. [_PAGE_USER][_PAGE_USER] - Page is accessible from user space.
4. [_PAGE_ACCESSED][_PAGE_ACCESSED] - Page has been accessed.
5. [_PAGE_DIRTY][_PAGE_DIRTY] - Page has been written to.
6. [_PAGE_PROTNONE][_PAGE_PROTNONE] - The page is resident, but not accessible.

* The bit referenced by `_PAGE_PROTNONE` is, in pentium III processors and
  higher, referred to as the 'Page Attribute Table' (PAT) bit and used to
  indicate the size of the page that the PTE is referencing. Equivalently, this
  bit is referred to as the 'Page Size Extension' (PSE) bit.

* Linux doesn't use either PAT or PSE in userspace, so this bit is free for
  other uses. It's used to fulfil cases like an [mprotect()][mprotect]'d region
  of memory set to `PROT_NONE` where the memory needs to resident, but
  inaccessible to userland. This is achieved by clearing `_PAGE_PRESENT` and
  setting `_PAGE_PROTNONE`.

* Since `_PAGE_PRESENT` is clear, the hardware will invoke a fault if userland
  tries to access it, but the [pte_present()][pte_present] macro will detect
  that `PAGE_PROTNONE` is defined so the kernel will know to keep this page
  resident.

## 3.3 Using Page Table Entries

* (See the cheat sheet for details on useful functions.)

[4level]:https://lwn.net/Articles/117749/
[mmu]:https://en.wikipedia.org/wiki/Memory_management_unit
[tlb]:https://en.wikipedia.org/wiki/Translation_lookaside_buffer
[mm_struct]:http://fxr.watson.org/fxr/source/include/linux/sched.h?v=linux-2.4.22#L206
[pgd_t]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22;#L42
[__flush_tlb]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L38
[pmd_t]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L41
[pte_t]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L40
[pgd_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L331
[pgd_index]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L327
[PGDIR_SHIFT/2lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L8
[PTRS_PER_PGD/2lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L9
[PGDIR_SHIFT/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L14
[PTRS_PER_PGD/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L15
[pmd_offset/2lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L55
[pmd_offset/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L72
[pgd_page]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-2level.h?v=linux-2.4.22#L52
[PAGE_MASK]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L7
[pgd_val]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L55
[__va]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L133
[PAE]:https://en.wikipedia.org/wiki/Physical_Address_Extension
[__pmd_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L336
[PMD_SHIFT/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L21
[PTRS_PER_PMD/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L22
[pte_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L342
[__pte_offset]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L340
[pmd_page]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L323
[pte_val/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L43
[pmd_val]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L54
[__PAGE_OFFSET]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L81
[pte_page]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L92
[page]:http://fxr.watson.org/fxr/source/include/linux/mm.h?v=linux-2.4.22#L154
[PAGE_SIZE]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L6
[PMD_SIZE]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L136
[PMD_MASK]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L137
[PGDIR_SIZE]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L138
[PGDIR_MASK]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L139
[PAGE_ALIGN]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L66
[PAGE_MASK]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L7
[PTRS_PER_PTE/3lvl]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable-3level.h?v=linux-2.4.22#L27
[pgprot_t]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L52
[__pte]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L58
[__pmd]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L59
[__pgd]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L60
[__pgprot]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L61
[pgprot_val]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L56
[PAGE_SHIFT]:http://fxr.watson.org/fxr/source/include/asm-i386/page.h?v=linux-2.4.22#L5
[_PAGE_PRESENT]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L187
[_PAGE_RW]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L188
[_PAGE_USER]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L189
[_PAGE_ACCESSED]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L192
[_PAGE_DIRTY]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L193
[_PAGE_PROTNONE]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L197
[mprotect]:http://man7.org/linux/man-pages/man2/mprotect.2.html
[pte_present]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L267
[pmd_none]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L270
[pmd_present]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L271
[pmd_clear]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L272
[pmd_bad]:http://fxr.watson.org/fxr/source/include/asm-i386/pgtable.h?v=linux-2.4.22#L273
